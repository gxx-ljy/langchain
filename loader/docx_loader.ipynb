{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb4693b-750f-424f-80f1-b41c7c250aa9",
   "metadata": {},
   "source": [
    "# docx\n",
    "\n",
    "python-docx加载为文本形式，如果使用langchain做分割，只支持以文本形式分割，不支持以文档形式分割，适合自定义milvus表结构的情况，自行转向量，不适合直接用langchain做向量化插入数据库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8e7007-30ea-4af0-9396-86488c970b1f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-01T02:23:29.716924Z",
     "iopub.status.busy": "2023-12-01T02:23:29.716924Z",
     "iopub.status.idle": "2023-12-01T02:23:29.866964Z",
     "shell.execute_reply": "2023-12-01T02:23:29.865825Z",
     "shell.execute_reply.started": "2023-12-01T02:23:29.716924Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.docx\n",
      "1.5 神经网络深度学习发展史\n",
      "神经网络的发展历史中有过三次热潮，分别发展在 20 世纪 40 年代到 60 年代，20 世纪80 年代到 90 年代，以及 2006 年至今。每一次神经网络的热潮都伴随着人工智能的兴起，人工智能和神经网络一直以来都有着非常密切的关系。\n",
      "\n",
      "1.5.1 神经网络诞生-20 世纪 40-60 年代\n",
      "1943 年，神经病学家和神经元解剖学家 W.S.McCulloch 和数学家 W.A.Pitts 在生物物理学期刊发表文章提出神经元的数学描述和结构。并且证明了只要有足够的简单神经元，在这些神经元互相连接并同步运行的情况下，可以模拟任何计算函数，这种神经元的数学模型称为 MP 模型。该模型把神经元的动作描述为：\n",
      "1.神经元的活动表现为兴奋或抑制的二值变化；\n",
      "2.任何兴奋性突触输入激励后，使神经元兴奋；\n",
      "3.任何抑制性突触有输入激励后，使神经元抑制；\n",
      "4.突触的值不随时间改变；5.突触从感知输入到传送出一个输出脉冲的延时时间是 0.5ms。\n",
      "\n",
      "尽管现在看来 M-P 模型过于简单，并且观点也不是完全正确，不过这个模型被认为是第一个仿生学的神经网络模型，他们提出的很多观点一直沿用至今，比如说他们认为神经元有两种状态，要不就是兴奋，要不就是抑制。这跟后面要提到的单层感知器非常类似，单层感知器的输出要不就是 0 要不就是 1。他们最重要的贡献就是开创了神经网络这个研究方向，为今天神经网络的发展奠定了基础。\n",
      "\n",
      "1949 年 ， 另 一 位 心 理 学 家 Donald Olding Hebb 在 他 的一本 名 为《The organization of behavior: A neuropsychological theory》[4]的书提出了 Hebb 算法。他也是首先提出“连接主义”（connectionism）这一名词的人之一，这个名词的含义是大脑的活动是靠脑细胞的组合连接实现的。Hebb 认为，如果源和目的神经元均被激活兴奋时，它们之间突触的连接强度将会增强。他指出在神经网络中，信息存储在连接权值中。并提出假设神经元 A 到神经元 B 连接权与从 B 到 A 的连接权是相同的。他这里提到的这个权值的思想也被应用到了我们目前所使用的神经网络中，我们通过调节神经元之间的连接权值来得到不同的神经网络模型，实现不同的应用。虽然这些理论在今天看来是理所当然的，不过在当时看来这是一种全新的想法，算得上是开创性的理论。\n",
      "\n",
      "1958 年，计算机学家 Frank Rosenblatt 提出了一种神经网络结构，称为感知器(Perceptron)。他提出的这个感知器可能是世界上第一个真正意义上的人工神经网络。感知器提出之后在 60 年代就掀起了神经网络研究的第一次热潮。很多人都认为只要使用成千上万的神经元，他们就能解决一切问题。现在看来可能会让人感觉 too young too naive，不过感知器在当时确实是影响非凡。这股感知器热潮持续了 10 年，直到 1969 年，人工智能的创始人之一的 M.Minsky 和S.Papert 出版了一本名为《感知器》[5]的书，书中指出简单神经网络只能运用于线性问题的求解，能够求解非线性问题的网络应具有隐层，而从理论上还不能证明将感知器模型扩展到多层网络是有意义的。由于 Minsky 在学术界的地位和影响，其悲观论点极大地影响了当时的人工神经网络研究，为刚刚燃起希望之火的人工神经网络泼了一大盘冷水。这本书出版不不久之后，几乎所有为神经网络提供的研究基金都枯竭了，没有人愿意把钱浪费在没有意义的事情上。\n",
      "\n",
      "1.5.2 神经网络复兴-20 世纪 80-90 年代\n",
      "\n",
      "1982 年，美国加州理工学院的优秀物理学家 John J.Hopfield 博士提出了 Hopfield 神经网络。Hopfield 神经网络引用了物理力学的分析方法，把网络作为一种动态系统并研究这种网络动态系统的稳定性。\n",
      "\n",
      "1985 年，G.E.Hinton 和 T.J.Sejnowski 借助统计物理学的概念和方法提出了一种随机神经网络模型——玻尔兹曼机(Boltzmann Machine)。一年后他们又改进了模型，提出了受限玻尔兹曼机(Restricted Boltzmann Machine)。\n",
      "\n",
      "1986 年，Rumelhart，Hinton，Williams 提出了 BP(Back Propagation)算法[6]（多层感知器的误差反向传播算法）。到今天为止，这种多层感知器的误差反向传播算法还是非常基础的算法，凡是学神经网络的人，必然要学习 BP 算法。我们现在的深度网络模型基本上都是在这个算法的基础上发展出来的。使用 BP 算法的多层神经网络也称为 BP 神经网络（Back Propagation Neural network）。BP 神经网络主要指的是 20 世纪 80-90 年代使用 BP 算法的神经网络，虽然现在的深度学习也用 BP 算法，不过网络名称已经不叫 BP 神经网络了。早期的 BP 神经网络的神经元层数不能太多，一旦网络层数过多，就会使得网络无法训练，具体原因在后面的章节中会详细说明。Hopfield 神经网络，玻尔兹曼机以及受限玻尔兹曼机由于目前已经较少使用，所以本书后面章节不再详细介绍这三种网络。\n",
      "\n",
      "1.5.3 深度学习-2006 年至今\n",
      "\n",
      "2006 年，多伦多大学的教授 Geoffrey Hinton 提出了深度学习。他在世界顶级学术期刊《Science》上发表了一篇论文《Reducing the dimensionality of data with neural networks》[7]，论文中提出了两个观点：①多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原始数据有更本质的代表性，这将大大便于分类和可视化问题；②对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。\n",
      "\n",
      "Hinton 在论文中提出了一种新的网络结构深度置信网络（Deep Belief Net：DBN），这种网络使得训练深层的神经网络成为可能。深度置信网络由于目前已经较少使用，所以本书后面章节不再详细介绍这种网络。\n",
      "\n",
      "2012 年，Hinton 课题组为了证明深度学习的潜力，首次参加 ImageNet 图像识别比赛，通过 CNN 网络 AlexNet 一举夺得冠军。也正是由于该比赛，CNN 吸引了众多研究者的注意。\n",
      "\n",
      "2014 年，香港中文大学教授汤晓鸥领导的计算机视觉研究组开发了名为 DeepID 的深度学习模型， 在 LFW (Labeled Faces in the Wild，人脸识别使用非常广泛的测试基准)数据库上获得了 99.15%的识别率，人用肉眼在 LFW 上的识别率为 97.52%，深度学习在学术研究层面上已经超过了人用肉眼的识别。\n",
      "\n",
      "2016 年 3 月人工智能围棋比赛，由位于英国伦敦的谷歌（Google）旗下 DeepMind 公司的开发的 AlphaGo 战胜了世界围棋冠军、职业九段选手李世石，并以 4:1 的总比分获胜。\n",
      "\n",
      "2018 年 6 月，OpenAI 的研究人员开发了一种技术，可以在未标记的文本上训练 AI，可以大量减少人工标注的时间。几个月后谷歌推出了一个名为 BERT 的模型，该模型在学习了几百 万 个 句 子以后学会了 如 何 预测漏 掉 的 单 词 。 在 多 项 NLP (Natural Language Processing) 测试中，它的表现都接近人类。\n",
      "\n",
      "2020 年 6 月，OpenAI 发布了有史以来最大的 NLP 模型 GPT-3，GPT-3 模型参数达到了 1750 亿个参数，模型训练花费了上千万美元。GPT-3 训练方法很简单，但是却非常全能，可以完成填空，翻译，问答，阅读理解，数学计算，语法纠错等多项任务。随着 NLP 技术的发展，相信在将来 AI 可以逐渐理解我们的语言，跟我们进行顺畅的对话，甚至成为我们的保姆，老师或朋友。\n",
      "\n",
      "今天，人脸识别技术已经应用在了我们生活的方方面面，比如上下班打卡，飞机高铁出行，出门住酒店，刷脸支付等。我们已经离不开深度学习技术，而深度学习技术仍在快速发展中。\n",
      "\n",
      "1.6 深度学习领域重要人物\n",
      "\n",
      "深度学习领域有很多做出过卓越贡献的大师，下面简单介绍几位。前面的 3 位大师Geoffrey Hinton、Yann LeCun、Yoshua Bengio 江湖人称“深度学习三巨头”，为了表彰 3 位大师对于神经网络深度学习领域的贡献，2018 年计算机领域最高奖项图灵奖颁给了他们。\n",
      "\n",
      "1.Geoffrey Hinton\n",
      "\n",
      "英国出生的计算机学家和心理学家，以其在神经网络方面的贡献闻名。Hinton 是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。目前担任多伦多大学计算机科学系教授。2013 年 3 月加入 Google，领导 Google Brain 项目。Hinton 被人们称为“深度学习教父”，可以说是目前对深度学习领域影响最大的人。而且如今在深度学习领域活跃的大师，有很多都是他的弟子，可以说是桃李满天下。\n",
      "\n",
      "2.Yann LeCun\n",
      "\n",
      "法国出生的计算机科学家，他最著名的工作是光学字符识别和计算机视觉上使用卷积神经网络（CNN），他也被称为卷积网络之父。曾在多伦多大学跟随 Geoffrey Hinton 做博士后。1988 年加入贝尔实验室，在贝尔实验\n",
      "室工作期间开发了一套能够识别手写数字的卷积神经网络系统，并把它命名为 LeNet。这个系统能自动识别银行支票。\n",
      "2003 年去了纽约大学担任教授，现在是纽约大学终身教授。\n",
      "2013 年 12 月加入了 Facebook，成为 Facebook 人工智能实验室的第一任主任。\n",
      "\n",
      "3.Yoshua Bengio\n",
      "\n",
      "毕业于麦吉尔大学，在 MIT 和贝尔实验室做过博士后研究员，自 1993 年之后就在蒙特利尔大学任教。在预训练问题，自动编码器降噪等领域做出重大贡献。\n",
      "这“三巨头”中的前两人早已投身工业界，而 Bengio 仍留在学术界教书，他曾说过：“我留在学术圈是为全人类作贡献，而不是为某一公司赚钱”。他说这句话一定是因为他很有钱，开个玩笑。每个领域的发展不仅需要做前沿的研究，还需要不断培养新的新鲜血液加入到这个行业中，所以如果大学教授都去工作的话，上课教书的人就少了。所以 Bengio 能留在学术圈，对行业的发展也是一件好事。\n",
      "2017 年初 Bengio 选择加入微软成为战略顾问。他表示不希望有一家或者两家公司（他指的显然是 Google 和 Facebook）成为人工智能变革中的唯一大玩家，这对研究社区没有好处，对人类也没有好处。\n",
      "\n",
      "4.Andrew Ng（吴恩达）\n",
      "\n",
      "Andrew Ng 是美籍华人，曾经是斯坦福大学计算机科学系和电气工程系的副教授，斯坦福人工智能实验室主任。他还与 Daphne Koller 一起创建了在线教育平台 Coursera。\n",
      "2011 年，Andrew Ng 在 Google 创建了 Google Brain 项目，通过分布式集群计算机开发超大规模的人工神经网络。\n",
      "2014 年 5 月，Andrew Ng 加入百度，负责百度大脑计划，并担任百度公司首席科学家。\n",
      "2017 年 3 月，Andrew Ng 从百度离职，目前自己创业。\n",
      "LeCun 是 Hinton 的 博士生，另一位人工智能大师 Jordan 曾经申请过 Hinton 的博士生，Bengio 是 Jordan 的博士后，Andrew Ng 是 Jordan 的博士生，LeCun 与 Bengio 曾经是同事。这个圈子很小，大家都认识，这几位大师互相之间有着很深的渊源。\n",
      "\n",
      "曾经神经网络的圈子很小，基本上入了这个圈以后就没什么前途了。正是由于这个圈子里的这些大师前辈们的不懈努力，把神经网络算法不断优化，才有了今天的深度学习和今天人工智能的新局面。\n",
      "\n",
      "1.7 新一轮人工智能爆发的三要素\n",
      "\n",
      "这一轮人工智能大爆发的主要原因有 3 个，深度学习算法，大数据，以及高性能计算。深度学习算法 —— 之前人工智能领域的实际应用主要是使用传统的机器学习算法，虽然这些传统的机器学习算法在很多领域都取得了不错的效果，不过仍然有非常大的提升空间。\n",
      "\n",
      "深度学习出现后，计算机视觉，自然语言处理，语音识别等领域都取得了非常大的进步。大数据 —— 如果把人工智能比喻成一个火箭，那么这个火箭需要发射升空，它的燃料就是大数据。以前在实验室环境下很难收集到足够多的样本，现在的数据相对以前在数量、覆盖性和全面性方面都获得了大幅提升。一般来说深度学习模型想要获得好的效果，就需要把大量的数据放到模型中进行训练。\n",
      "\n",
      "高性能计算 —— 以前高性能计算大家用的是 CPU 集群，现在做深度学习都是用GPU(Graphics Processing Unit)或 TPU(Tensor Processing Unit)。想要使用大量的数据来训练复杂的深度学习模型那就必须要具备高性能计算能力。GPU 就是我们日常所说的显卡，平时主要用于打游戏。但是 GPU 不仅可以用于打游戏，还可以用来训练模型，性价比很高，买显卡的理由又多了一个。如果只是使用几个 CPU 来训练一个复杂模型可能会需要花费几周\n",
      "甚至几个月的时间。把数百块 GPU 连接起来做成集群，用这些集群来训练模型，原来一个月才能训练出来的网络，可以加速到几个小时甚至几分钟就能训练完，可以大大减少模型训练时间。TPU 是谷歌专门为机器学习量身定做的处理器，执行每个操作所需的晶体管数量更少，效率更高。\n",
      "\n",
      "工欲善其事，必先利其器。下一章节我们将介绍如何搭建 python 开发环境，为我们后续的学习做准。\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    content = [file_path]\n",
    "    for para in doc.paragraphs:\n",
    "        content.append(para.text)\n",
    "    return '\\n'.join(content)\n",
    "\n",
    "file_path = 'test.docx'\n",
    "content = read_docx(file_path)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f166a77-8f20-495f-989f-6f364d9cf173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:25:42.619030Z",
     "iopub.status.busy": "2023-12-01T02:25:42.619030Z",
     "iopub.status.idle": "2023-12-01T02:25:42.628676Z",
     "shell.execute_reply": "2023-12-01T02:25:42.627693Z",
     "shell.execute_reply.started": "2023-12-01T02:25:42.619030Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8766e863-9fe2-4219-82d7-003dc306c582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:25:42.929683Z",
     "iopub.status.busy": "2023-12-01T02:25:42.928889Z",
     "iopub.status.idle": "2023-12-01T02:25:42.949500Z",
     "shell.execute_reply": "2023-12-01T02:25:42.949500Z",
     "shell.execute_reply.started": "2023-12-01T02:25:42.929683Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m docs1 \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 对比下面的 split_text\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Users\\gxx_l\\anaconda3\\envs\\chat-env\\lib\\site-packages\\langchain\\text_splitter.py:159\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m    157\u001b[0m texts, metadatas \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m--> 159\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m)\n\u001b[0;32m    160\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "docs1 = text_splitter.split_documents(content) # 对比下面的 split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a19c33-2ec2-4b36-8e62-c6671602c0f6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-01T02:25:43.489427Z",
     "iopub.status.busy": "2023-12-01T02:25:43.489427Z",
     "iopub.status.idle": "2023-12-01T02:25:43.503486Z",
     "shell.execute_reply": "2023-12-01T02:25:43.502459Z",
     "shell.execute_reply.started": "2023-12-01T02:25:43.489427Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.docx\\n1.5 神经网络深度学习发展史\\n神经网络的发展历史中有过三次热潮，分别发展在 20 世纪 40 年代到 60 年代，20 世纪80 年代到 90 年代，以及 2006 年至今。每一次神经网络的热潮都伴随着人工智能的兴起，人工智能和神经网络一直以来都有着非常密切的关系。\\n\\n1.5.1 神经网络诞生-20 世纪 40-60 年代\\n1943 年，神经病学家和神经元解剖学家 W.S.McCulloch 和数学家 W.A.Pitts 在生物物理学期刊发表文章提出神经元的数学描述和结构。并且证明了只要有足够的简单神经元，在这些神经元互相连接并同步运行的情况下，可以模拟任何计算函数，这种神经元的数学模型称为 MP 模型。该模型把神经元的动作描述为：\\n1.神经元的活动表现为兴奋或抑制的二值变化；\\n2.任何兴奋性突触输入激励后，使神经元兴奋；\\n3.任何抑制性突触有输入激励后，使神经元抑制；\\n4.突触的值不随时间改变；5.突触从感知输入到传送出一个输出脉冲的延时时间是 0.5ms。', '尽管现在看来 M-P 模型过于简单，并且观点也不是完全正确，不过这个模型被认为是第一个仿生学的神经网络模型，他们提出的很多观点一直沿用至今，比如说他们认为神经元有两种状态，要不就是兴奋，要不就是抑制。这跟后面要提到的单层感知器非常类似，单层感知器的输出要不就是 0 要不就是 1。他们最重要的贡献就是开创了神经网络这个研究方向，为今天神经网络的发展奠定了基础。', '1949 年 ， 另 一 位 心 理 学 家 Donald Olding Hebb 在 他 的一本 名 为《The organization of behavior: A neuropsychological theory》[4]的书提出了 Hebb 算法。他也是首先提出“连接主义”（connectionism）这一名词的人之一，这个名词的含义是大脑的活动是靠脑细胞的组合连接实现的。Hebb 认为，如果源和目的神经元均被激活兴奋时，它们之间突触的连接强度将会增强。他指出在神经网络中，信息存储在连接权值中。并提出假设神经元 A 到神经元 B 连接权与从 B 到 A 的连接权是相同的。他这里提到的这个权值的思想也被应用到了我们目前所使用的神经网络中，我们通过调节神经元之间的连接权值来得到不同的神经网络模型，实现不同的应用。虽然这些理论在今天看来是理所当然的，不过在当时看来这是一种全新的想法，算得上是开创性的理论。', '1958 年，计算机学家 Frank Rosenblatt 提出了一种神经网络结构，称为感知器(Perceptron)。他提出的这个感知器可能是世界上第一个真正意义上的人工神经网络。感知器提出之后在 60 年代就掀起了神经网络研究的第一次热潮。很多人都认为只要使用成千上万的神经元，他们就能解决一切问题。现在看来可能会让人感觉 too young too naive，不过感知器在当时确实是影响非凡。这股感知器热潮持续了 10 年，直到 1969 年，人工智能的创始人之一的 M.Minsky 和S.Papert 出版了一本名为《感知器》[5]的书，书中指出简单神经网络只能运用于线性问题的求解，能够求解非线性问题的网络应具有隐层，而从理论上还不能证明将感知器模型扩展到多层网络是有意义的。由于 Minsky 在学术界的地位和影响，其悲观论点极大地影响了当时的人工神经网络研究，为刚刚燃起希望之火的人工神经网络泼了一大盘冷水。这本书出版不不久之后，几乎所有为神经网络提供的研究基金都枯竭了，没有人愿意把钱浪费在没有意义的事情上。\\n\\n1.5.2 神经网络复兴-20 世纪 80-90 年代', '1.5.2 神经网络复兴-20 世纪 80-90 年代\\n\\n1982 年，美国加州理工学院的优秀物理学家 John J.Hopfield 博士提出了 Hopfield 神经网络。Hopfield 神经网络引用了物理力学的分析方法，把网络作为一种动态系统并研究这种网络动态系统的稳定性。\\n\\n1985 年，G.E.Hinton 和 T.J.Sejnowski 借助统计物理学的概念和方法提出了一种随机神经网络模型——玻尔兹曼机(Boltzmann Machine)。一年后他们又改进了模型，提出了受限玻尔兹曼机(Restricted Boltzmann Machine)。', '1986 年，Rumelhart，Hinton，Williams 提出了 BP(Back Propagation)算法[6]（多层感知器的误差反向传播算法）。到今天为止，这种多层感知器的误差反向传播算法还是非常基础的算法，凡是学神经网络的人，必然要学习 BP 算法。我们现在的深度网络模型基本上都是在这个算法的基础上发展出来的。使用 BP 算法的多层神经网络也称为 BP 神经网络（Back Propagation Neural network）。BP 神经网络主要指的是 20 世纪 80-90 年代使用 BP 算法的神经网络，虽然现在的深度学习也用 BP 算法，不过网络名称已经不叫 BP 神经网络了。早期的 BP 神经网络的神经元层数不能太多，一旦网络层数过多，就会使得网络无法训练，具体原因在后面的章节中会详细说明。Hopfield 神经网络，玻尔兹曼机以及受限玻尔兹曼机由于目前已经较少使用，所以本书后面章节不再详细介绍这三种网络。\\n\\n1.5.3 深度学习-2006 年至今', '1.5.3 深度学习-2006 年至今\\n\\n2006 年，多伦多大学的教授 Geoffrey Hinton 提出了深度学习。他在世界顶级学术期刊《Science》上发表了一篇论文《Reducing the dimensionality of data with neural networks》[7]，论文中提出了两个观点：①多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原始数据有更本质的代表性，这将大大便于分类和可视化问题；②对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。\\n\\nHinton 在论文中提出了一种新的网络结构深度置信网络（Deep Belief Net：DBN），这种网络使得训练深层的神经网络成为可能。深度置信网络由于目前已经较少使用，所以本书后面章节不再详细介绍这种网络。', '2012 年，Hinton 课题组为了证明深度学习的潜力，首次参加 ImageNet 图像识别比赛，通过 CNN 网络 AlexNet 一举夺得冠军。也正是由于该比赛，CNN 吸引了众多研究者的注意。\\n\\n2014 年，香港中文大学教授汤晓鸥领导的计算机视觉研究组开发了名为 DeepID 的深度学习模型， 在 LFW (Labeled Faces in the Wild，人脸识别使用非常广泛的测试基准)数据库上获得了 99.15%的识别率，人用肉眼在 LFW 上的识别率为 97.52%，深度学习在学术研究层面上已经超过了人用肉眼的识别。\\n\\n2016 年 3 月人工智能围棋比赛，由位于英国伦敦的谷歌（Google）旗下 DeepMind 公司的开发的 AlphaGo 战胜了世界围棋冠军、职业九段选手李世石，并以 4:1 的总比分获胜。', '2018 年 6 月，OpenAI 的研究人员开发了一种技术，可以在未标记的文本上训练 AI，可以大量减少人工标注的时间。几个月后谷歌推出了一个名为 BERT 的模型，该模型在学习了几百 万 个 句 子以后学会了 如 何 预测漏 掉 的 单 词 。 在 多 项 NLP (Natural Language Processing) 测试中，它的表现都接近人类。\\n\\n2020 年 6 月，OpenAI 发布了有史以来最大的 NLP 模型 GPT-3，GPT-3 模型参数达到了 1750 亿个参数，模型训练花费了上千万美元。GPT-3 训练方法很简单，但是却非常全能，可以完成填空，翻译，问答，阅读理解，数学计算，语法纠错等多项任务。随着 NLP 技术的发展，相信在将来 AI 可以逐渐理解我们的语言，跟我们进行顺畅的对话，甚至成为我们的保姆，老师或朋友。\\n\\n今天，人脸识别技术已经应用在了我们生活的方方面面，比如上下班打卡，飞机高铁出行，出门住酒店，刷脸支付等。我们已经离不开深度学习技术，而深度学习技术仍在快速发展中。\\n\\n1.6 深度学习领域重要人物', '1.6 深度学习领域重要人物\\n\\n深度学习领域有很多做出过卓越贡献的大师，下面简单介绍几位。前面的 3 位大师Geoffrey Hinton、Yann LeCun、Yoshua Bengio 江湖人称“深度学习三巨头”，为了表彰 3 位大师对于神经网络深度学习领域的贡献，2018 年计算机领域最高奖项图灵奖颁给了他们。\\n\\n1.Geoffrey Hinton\\n\\n英国出生的计算机学家和心理学家，以其在神经网络方面的贡献闻名。Hinton 是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。目前担任多伦多大学计算机科学系教授。2013 年 3 月加入 Google，领导 Google Brain 项目。Hinton 被人们称为“深度学习教父”，可以说是目前对深度学习领域影响最大的人。而且如今在深度学习领域活跃的大师，有很多都是他的弟子，可以说是桃李满天下。\\n\\n2.Yann LeCun', '2.Yann LeCun\\n\\n法国出生的计算机科学家，他最著名的工作是光学字符识别和计算机视觉上使用卷积神经网络（CNN），他也被称为卷积网络之父。曾在多伦多大学跟随 Geoffrey Hinton 做博士后。1988 年加入贝尔实验室，在贝尔实验\\n室工作期间开发了一套能够识别手写数字的卷积神经网络系统，并把它命名为 LeNet。这个系统能自动识别银行支票。\\n2003 年去了纽约大学担任教授，现在是纽约大学终身教授。\\n2013 年 12 月加入了 Facebook，成为 Facebook 人工智能实验室的第一任主任。\\n\\n3.Yoshua Bengio', '3.Yoshua Bengio\\n\\n毕业于麦吉尔大学，在 MIT 和贝尔实验室做过博士后研究员，自 1993 年之后就在蒙特利尔大学任教。在预训练问题，自动编码器降噪等领域做出重大贡献。\\n这“三巨头”中的前两人早已投身工业界，而 Bengio 仍留在学术界教书，他曾说过：“我留在学术圈是为全人类作贡献，而不是为某一公司赚钱”。他说这句话一定是因为他很有钱，开个玩笑。每个领域的发展不仅需要做前沿的研究，还需要不断培养新的新鲜血液加入到这个行业中，所以如果大学教授都去工作的话，上课教书的人就少了。所以 Bengio 能留在学术圈，对行业的发展也是一件好事。\\n2017 年初 Bengio 选择加入微软成为战略顾问。他表示不希望有一家或者两家公司（他指的显然是 Google 和 Facebook）成为人工智能变革中的唯一大玩家，这对研究社区没有好处，对人类也没有好处。\\n\\n4.Andrew Ng（吴恩达）', '4.Andrew Ng（吴恩达）\\n\\nAndrew Ng 是美籍华人，曾经是斯坦福大学计算机科学系和电气工程系的副教授，斯坦福人工智能实验室主任。他还与 Daphne Koller 一起创建了在线教育平台 Coursera。\\n2011 年，Andrew Ng 在 Google 创建了 Google Brain 项目，通过分布式集群计算机开发超大规模的人工神经网络。\\n2014 年 5 月，Andrew Ng 加入百度，负责百度大脑计划，并担任百度公司首席科学家。\\n2017 年 3 月，Andrew Ng 从百度离职，目前自己创业。\\nLeCun 是 Hinton 的 博士生，另一位人工智能大师 Jordan 曾经申请过 Hinton 的博士生，Bengio 是 Jordan 的博士后，Andrew Ng 是 Jordan 的博士生，LeCun 与 Bengio 曾经是同事。这个圈子很小，大家都认识，这几位大师互相之间有着很深的渊源。', '曾经神经网络的圈子很小，基本上入了这个圈以后就没什么前途了。正是由于这个圈子里的这些大师前辈们的不懈努力，把神经网络算法不断优化，才有了今天的深度学习和今天人工智能的新局面。\\n\\n1.7 新一轮人工智能爆发的三要素\\n\\n这一轮人工智能大爆发的主要原因有 3 个，深度学习算法，大数据，以及高性能计算。深度学习算法 —— 之前人工智能领域的实际应用主要是使用传统的机器学习算法，虽然这些传统的机器学习算法在很多领域都取得了不错的效果，不过仍然有非常大的提升空间。\\n\\n深度学习出现后，计算机视觉，自然语言处理，语音识别等领域都取得了非常大的进步。大数据 —— 如果把人工智能比喻成一个火箭，那么这个火箭需要发射升空，它的燃料就是大数据。以前在实验室环境下很难收集到足够多的样本，现在的数据相对以前在数量、覆盖性和全面性方面都获得了大幅提升。一般来说深度学习模型想要获得好的效果，就需要把大量的数据放到模型中进行训练。', '高性能计算 —— 以前高性能计算大家用的是 CPU 集群，现在做深度学习都是用GPU(Graphics Processing Unit)或 TPU(Tensor Processing Unit)。想要使用大量的数据来训练复杂的深度学习模型那就必须要具备高性能计算能力。GPU 就是我们日常所说的显卡，平时主要用于打游戏。但是 GPU 不仅可以用于打游戏，还可以用来训练模型，性价比很高，买显卡的理由又多了一个。如果只是使用几个 CPU 来训练一个复杂模型可能会需要花费几周\\n甚至几个月的时间。把数百块 GPU 连接起来做成集群，用这些集群来训练模型，原来一个月才能训练出来的网络，可以加速到几个小时甚至几分钟就能训练完，可以大大减少模型训练时间。TPU 是谷歌专门为机器学习量身定做的处理器，执行每个操作所需的晶体管数量更少，效率更高。\\n\\n工欲善其事，必先利其器。下一章节我们将介绍如何搭建 python 开发环境，为我们后续的学习做准。']\n"
     ]
    }
   ],
   "source": [
    "docs2 = text_splitter.split_text(content) # 对比上面的 split_documents\n",
    "print(docs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc64b0b-9df1-463e-a291-bc3579a50e36",
   "metadata": {},
   "source": [
    "# UnstructuredWordDocumentLoader\n",
    "\n",
    "UnstructuredWordDocumentLoader支持使用mode=\"elements\"参数在加载文档时就对文档进行分割。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a5d248-8fa9-48ce-ab5f-01b3576d0881",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-01T02:30:48.558738Z",
     "iopub.status.busy": "2023-12-01T02:30:48.557536Z",
     "iopub.status.idle": "2023-12-01T02:30:52.111539Z",
     "shell.execute_reply": "2023-12-01T02:30:52.111539Z",
     "shell.execute_reply.started": "2023-12-01T02:30:48.558738Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1.5 神经网络深度学习发展史\\n\\n神经网络的发展历史中有过三次热潮，分别发展在 20 世纪 40 年代到 60 年代，20 世纪80 年代到 90 年代，以及 2006 年至今。每一次神经网络的热潮都伴随着人工智能的兴起，人工智能和神经网络一直以来都有着非常密切的关系。\\n\\n1.5.1 神经网络诞生-20 世纪 40-60 年代\\n\\n1943 年，神经病学家和神经元解剖学家 W.S.McCulloch 和数学家 W.A.Pitts 在生物物理学期刊发表文章提出神经元的数学描述和结构。并且证明了只要有足够的简单神经元，在这些神经元互相连接并同步运行的情况下，可以模拟任何计算函数，这种神经元的数学模型称为 MP 模型。该模型把神经元的动作描述为：\\n\\n1.神经元的活动表现为兴奋或抑制的二值变化；\\n\\n2.任何兴奋性突触输入激励后，使神经元兴奋；\\n\\n3.任何抑制性突触有输入激励后，使神经元抑制；\\n\\n4.突触的值不随时间改变；5.突触从感知输入到传送出一个输出脉冲的延时时间是 0.5ms。\\n\\n尽管现在看来 M-P 模型过于简单，并且观点也不是完全正确，不过这个模型被认为是第一个仿生学的神经网络模型，他们提出的很多观点一直沿用至今，比如说他们认为神经元有两种状态，要不就是兴奋，要不就是抑制。这跟后面要提到的单层感知器非常类似，单层感知器的输出要不就是 0 要不就是 1。他们最重要的贡献就是开创了神经网络这个研究方向，为今天神经网络的发展奠定了基础。\\n\\n1949 年 ， 另 一 位 心 理 学 家 Donald Olding Hebb 在 他 的一本 名 为《The organization of behavior: A neuropsychological theory》[4]的书提出了 Hebb 算法。他也是首先提出“连接主义”（connectionism）这一名词的人之一，这个名词的含义是大脑的活动是靠脑细胞的组合连接实现的。Hebb 认为，如果源和目的神经元均被激活兴奋时，它们之间突触的连接强度将会增强。他指出在神经网络中，信息存储在连接权值中。并提出假设神经元 A 到神经元 B 连接权与从 B 到 A 的连接权是相同的。他这里提到的这个权值的思想也被应用到了我们目前所使用的神经网络中，我们通过调节神经元之间的连接权值来得到不同的神经网络模型，实现不同的应用。虽然这些理论在今天看来是理所当然的，不过在当时看来这是一种全新的想法，算得上是开创性的理论。\\n\\n1958 年，计算机学家 Frank Rosenblatt 提出了一种神经网络结构，称为感知器(Perceptron)。他提出的这个感知器可能是世界上第一个真正意义上的人工神经网络。感知器提出之后在 60 年代就掀起了神经网络研究的第一次热潮。很多人都认为只要使用成千上万的神经元，他们就能解决一切问题。现在看来可能会让人感觉 too young too naive，不过感知器在当时确实是影响非凡。这股感知器热潮持续了 10 年，直到 1969 年，人工智能的创始人之一的 M.Minsky 和S.Papert 出版了一本名为《感知器》[5]的书，书中指出简单神经网络只能运用于线性问题的求解，能够求解非线性问题的网络应具有隐层，而从理论上还不能证明将感知器模型扩展到多层网络是有意义的。由于 Minsky 在学术界的地位和影响，其悲观论点极大地影响了当时的人工神经网络研究，为刚刚燃起希望之火的人工神经网络泼了一大盘冷水。这本书出版不不久之后，几乎所有为神经网络提供的研究基金都枯竭了，没有人愿意把钱浪费在没有意义的事情上。\\n\\n\\n\\n1.5.2 神经网络复兴-20 世纪 80-90 年代\\n\\n1982 年，美国加州理工学院的优秀物理学家 John J.Hopfield 博士提出了 Hopfield 神经网络。Hopfield 神经网络引用了物理力学的分析方法，把网络作为一种动态系统并研究这种网络动态系统的稳定性。\\n\\n1985 年，G.E.Hinton 和 T.J.Sejnowski 借助统计物理学的概念和方法提出了一种随机神经网络模型——玻尔兹曼机(Boltzmann Machine)。一年后他们又改进了模型，提出了受限玻尔兹曼机(Restricted Boltzmann Machine)。\\n\\n1986 年，Rumelhart，Hinton，Williams 提出了 BP(Back Propagation)算法[6]（多层感知器的误差反向传播算法）。到今天为止，这种多层感知器的误差反向传播算法还是非常基础的算法，凡是学神经网络的人，必然要学习 BP 算法。我们现在的深度网络模型基本上都是在这个算法的基础上发展出来的。使用 BP 算法的多层神经网络也称为 BP 神经网络（Back Propagation Neural network）。BP 神经网络主要指的是 20 世纪 80-90 年代使用 BP 算法的神经网络，虽然现在的深度学习也用 BP 算法，不过网络名称已经不叫 BP 神经网络了。早期的 BP 神经网络的神经元层数不能太多，一旦网络层数过多，就会使得网络无法训练，具体原因在后面的章节中会详细说明。Hopfield 神经网络，玻尔兹曼机以及受限玻尔兹曼机由于目前已经较少使用，所以本书后面章节不再详细介绍这三种网络。\\n\\n1.5.3 深度学习-2006 年至今\\n\\n2006 年，多伦多大学的教授 Geoffrey Hinton 提出了深度学习。他在世界顶级学术期刊《Science》上发表了一篇论文《Reducing the dimensionality of data with neural networks》[7]，论文中提出了两个观点：①多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原始数据有更本质的代表性，这将大大便于分类和可视化问题；②对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。\\n\\nHinton 在论文中提出了一种新的网络结构深度置信网络（Deep Belief Net：DBN），这种网络使得训练深层的神经网络成为可能。深度置信网络由于目前已经较少使用，所以本书后面章节不再详细介绍这种网络。\\n\\n2012 年，Hinton 课题组为了证明深度学习的潜力，首次参加 ImageNet 图像识别比赛，通过 CNN 网络 AlexNet 一举夺得冠军。也正是由于该比赛，CNN 吸引了众多研究者的注意。\\n\\n\\n\\n2014 年，香港中文大学教授汤晓鸥领导的计算机视觉研究组开发了名为 DeepID 的深度学习模型， 在 LFW (Labeled Faces in the Wild，人脸识别使用非常广泛的测试基准)数据库上获得了 99.15%的识别率，人用肉眼在 LFW 上的识别率为 97.52%，深度学习在学术研究层面上已经超过了人用肉眼的识别。\\n\\n2016 年 3 月人工智能围棋比赛，由位于英国伦敦的谷歌（Google）旗下 DeepMind 公司的开发的 AlphaGo 战胜了世界围棋冠军、职业九段选手李世石，并以 4:1 的总比分获胜。\\n\\n2018 年 6 月，OpenAI 的研究人员开发了一种技术，可以在未标记的文本上训练 AI，可以大量减少人工标注的时间。几个月后谷歌推出了一个名为 BERT 的模型，该模型在学习了几百 万 个 句 子以后学会了 如 何 预测漏 掉 的 单 词 。 在 多 项 NLP (Natural Language Processing) 测试中，它的表现都接近人类。\\n\\n2020 年 6 月，OpenAI 发布了有史以来最大的 NLP 模型 GPT-3，GPT-3 模型参数达到了 1750 亿个参数，模型训练花费了上千万美元。GPT-3 训练方法很简单，但是却非常全能，可以完成填空，翻译，问答，阅读理解，数学计算，语法纠错等多项任务。随着 NLP 技术的发展，相信在将来 AI 可以逐渐理解我们的语言，跟我们进行顺畅的对话，甚至成为我们的保姆，老师或朋友。\\n\\n今天，人脸识别技术已经应用在了我们生活的方方面面，比如上下班打卡，飞机高铁出行，出门住酒店，刷脸支付等。我们已经离不开深度学习技术，而深度学习技术仍在快速发展中。\\n\\n1.6 深度学习领域重要人物\\n\\n深度学习领域有很多做出过卓越贡献的大师，下面简单介绍几位。前面的 3 位大师Geoffrey Hinton、Yann LeCun、Yoshua Bengio 江湖人称“深度学习三巨头”，为了表彰 3 位大师对于神经网络深度学习领域的贡献，2018 年计算机领域最高奖项图灵奖颁给了他们。\\n\\n1.Geoffrey Hinton\\n\\n英国出生的计算机学家和心理学家，以其在神经网络方面的贡献闻名。Hinton 是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。目前担任多伦多大学计算机科学系教授。2013 年 3 月加入 Google，领导 Google Brain 项目。Hinton 被人们称为“深度学习教父”，可以说是目前对深度学习领域影响最大的人。而且如今在深度学习领域活跃的大师，有很多都是他的弟子，可以说是桃李满天下。\\n\\n2.Yann LeCun\\n\\n法国出生的计算机科学家，他最著名的工作是光学字符识别和计算机视觉上使用卷积神经网络（CNN），他也被称为卷积网络之父。曾在多伦多大学跟随 Geoffrey Hinton 做博士后。1988 年加入贝尔实验室，在贝尔实验\\n\\n\\n\\n室工作期间开发了一套能够识别手写数字的卷积神经网络系统，并把它命名为 LeNet。这个系统能自动识别银行支票。\\n\\n2003 年去了纽约大学担任教授，现在是纽约大学终身教授。\\n\\n2013 年 12 月加入了 Facebook，成为 Facebook 人工智能实验室的第一任主任。\\n\\n3.Yoshua Bengio\\n\\n毕业于麦吉尔大学，在 MIT 和贝尔实验室做过博士后研究员，自 1993 年之后就在蒙特利尔大学任教。在预训练问题，自动编码器降噪等领域做出重大贡献。\\n\\n这“三巨头”中的前两人早已投身工业界，而 Bengio 仍留在学术界教书，他曾说过：“我留在学术圈是为全人类作贡献，而不是为某一公司赚钱”。他说这句话一定是因为他很有钱，开个玩笑。每个领域的发展不仅需要做前沿的研究，还需要不断培养新的新鲜血液加入到这个行业中，所以如果大学教授都去工作的话，上课教书的人就少了。所以 Bengio 能留在学术圈，对行业的发展也是一件好事。\\n\\n2017 年初 Bengio 选择加入微软成为战略顾问。他表示不希望有一家或者两家公司（他指的显然是 Google 和 Facebook）成为人工智能变革中的唯一大玩家，这对研究社区没有好处，对人类也没有好处。\\n\\n4.Andrew Ng（吴恩达）\\n\\nAndrew Ng 是美籍华人，曾经是斯坦福大学计算机科学系和电气工程系的副教授，斯坦福人工智能实验室主任。他还与 Daphne Koller 一起创建了在线教育平台 Coursera。\\n\\n2011 年，Andrew Ng 在 Google 创建了 Google Brain 项目，通过分布式集群计算机开发超大规模的人工神经网络。\\n\\n2014 年 5 月，Andrew Ng 加入百度，负责百度大脑计划，并担任百度公司首席科学家。\\n\\n2017 年 3 月，Andrew Ng 从百度离职，目前自己创业。\\n\\nLeCun 是 Hinton 的 博士生，另一位人工智能大师 Jordan 曾经申请过 Hinton 的博士生，Bengio 是 Jordan 的博士后，Andrew Ng 是 Jordan 的博士生，LeCun 与 Bengio 曾经是同事。这个圈子很小，大家都认识，这几位大师互相之间有着很深的渊源。\\n\\n曾经神经网络的圈子很小，基本上入了这个圈以后就没什么前途了。正是由于这个圈子里的这些大师前辈们的不懈努力，把神经网络算法不断优化，才有了今天的深度学习和今天人工智能的新局面。\\n\\n1.7 新一轮人工智能爆发的三要素\\n\\n这一轮人工智能大爆发的主要原因有 3 个，深度学习算法，大数据，以及高性能计算。深度学习算法 —— 之前人工智能领域的实际应用主要是使用传统的机器学习算法，虽然这些传统的机器学习算法在很多领域都取得了不错的效果，不过仍然有非常大的提升空间。\\n\\n\\n\\n深度学习出现后，计算机视觉，自然语言处理，语音识别等领域都取得了非常大的进步。大数据 —— 如果把人工智能比喻成一个火箭，那么这个火箭需要发射升空，它的燃料就是大数据。以前在实验室环境下很难收集到足够多的样本，现在的数据相对以前在数量、覆盖性和全面性方面都获得了大幅提升。一般来说深度学习模型想要获得好的效果，就需要把大量的数据放到模型中进行训练。\\n\\n高性能计算 —— 以前高性能计算大家用的是 CPU 集群，现在做深度学习都是用GPU(Graphics Processing Unit)或 TPU(Tensor Processing Unit)。想要使用大量的数据来训练复杂的深度学习模型那就必须要具备高性能计算能力。GPU 就是我们日常所说的显卡，平时主要用于打游戏。但是 GPU 不仅可以用于打游戏，还可以用来训练模型，性价比很高，买显卡的理由又多了一个。如果只是使用几个 CPU 来训练一个复杂模型可能会需要花费几周\\n\\n甚至几个月的时间。把数百块 GPU 连接起来做成集群，用这些集群来训练模型，原来一个月才能训练出来的网络，可以加速到几个小时甚至几分钟就能训练完，可以大大减少模型训练时间。TPU 是谷歌专门为机器学习量身定做的处理器，执行每个操作所需的晶体管数量更少，效率更高。\\n\\n工欲善其事，必先利其器。下一章节我们将介绍如何搭建 python 开发环境，为我们后续的学习做准。', metadata={'source': 'test.docx'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "loader = UnstructuredWordDocumentLoader(\"test.docx\")\n",
    "documents1 = loader.load()\n",
    "documents1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1cb4bf5-83de-44d5-a488-6b32d0e63b14",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-01T02:31:11.531837Z",
     "iopub.status.busy": "2023-12-01T02:31:11.531837Z",
     "iopub.status.idle": "2023-12-01T02:31:11.542299Z",
     "shell.execute_reply": "2023-12-01T02:31:11.540660Z",
     "shell.execute_reply.started": "2023-12-01T02:31:11.531837Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1.5 神经网络深度学习发展史\\n\\n神经网络的发展历史中有过三次热潮，分别发展在 20 世纪 40 年代到 60 年代，20 世纪80 年代到 90 年代，以及 2006 年至今。每一次神经网络的热潮都伴随着人工智能的兴起，人工智能和神经网络一直以来都有着非常密切的关系。\\n\\n1.5.1 神经网络诞生-20 世纪 40-60 年代\\n\\n1943 年，神经病学家和神经元解剖学家 W.S.McCulloch 和数学家 W.A.Pitts 在生物物理学期刊发表文章提出神经元的数学描述和结构。并且证明了只要有足够的简单神经元，在这些神经元互相连接并同步运行的情况下，可以模拟任何计算函数，这种神经元的数学模型称为 MP 模型。该模型把神经元的动作描述为：\\n\\n1.神经元的活动表现为兴奋或抑制的二值变化；\\n\\n2.任何兴奋性突触输入激励后，使神经元兴奋；\\n\\n3.任何抑制性突触有输入激励后，使神经元抑制；\\n\\n4.突触的值不随时间改变；5.突触从感知输入到传送出一个输出脉冲的延时时间是 0.5ms。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='4.突触的值不随时间改变；5.突触从感知输入到传送出一个输出脉冲的延时时间是 0.5ms。\\n\\n尽管现在看来 M-P 模型过于简单，并且观点也不是完全正确，不过这个模型被认为是第一个仿生学的神经网络模型，他们提出的很多观点一直沿用至今，比如说他们认为神经元有两种状态，要不就是兴奋，要不就是抑制。这跟后面要提到的单层感知器非常类似，单层感知器的输出要不就是 0 要不就是 1。他们最重要的贡献就是开创了神经网络这个研究方向，为今天神经网络的发展奠定了基础。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1949 年 ， 另 一 位 心 理 学 家 Donald Olding Hebb 在 他 的一本 名 为《The organization of behavior: A neuropsychological theory》[4]的书提出了 Hebb 算法。他也是首先提出“连接主义”（connectionism）这一名词的人之一，这个名词的含义是大脑的活动是靠脑细胞的组合连接实现的。Hebb 认为，如果源和目的神经元均被激活兴奋时，它们之间突触的连接强度将会增强。他指出在神经网络中，信息存储在连接权值中。并提出假设神经元 A 到神经元 B 连接权与从 B 到 A 的连接权是相同的。他这里提到的这个权值的思想也被应用到了我们目前所使用的神经网络中，我们通过调节神经元之间的连接权值来得到不同的神经网络模型，实现不同的应用。虽然这些理论在今天看来是理所当然的，不过在当时看来这是一种全新的想法，算得上是开创性的理论。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1958 年，计算机学家 Frank Rosenblatt 提出了一种神经网络结构，称为感知器(Perceptron)。他提出的这个感知器可能是世界上第一个真正意义上的人工神经网络。感知器提出之后在 60 年代就掀起了神经网络研究的第一次热潮。很多人都认为只要使用成千上万的神经元，他们就能解决一切问题。现在看来可能会让人感觉 too young too naive，不过感知器在当时确实是影响非凡。这股感知器热潮持续了 10 年，直到 1969 年，人工智能的创始人之一的 M.Minsky 和S.Papert 出版了一本名为《感知器》[5]的书，书中指出简单神经网络只能运用于线性问题的求解，能够求解非线性问题的网络应具有隐层，而从理论上还不能证明将感知器模型扩展到多层网络是有意义的。由于 Minsky 在学术界的地位和影响，其悲观论点极大地影响了当时的人工神经网络研究，为刚刚燃起希望之火的人工神经网络泼了一大盘冷水。这本书出版不不久之后，几乎所有为神经网络提供的研究基金都枯竭了，没有人愿意把钱浪费在没有意义的事情上。\\n\\n\\n\\n1.5.2 神经网络复兴-20 世纪 80-90 年代', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1.5.2 神经网络复兴-20 世纪 80-90 年代\\n\\n1982 年，美国加州理工学院的优秀物理学家 John J.Hopfield 博士提出了 Hopfield 神经网络。Hopfield 神经网络引用了物理力学的分析方法，把网络作为一种动态系统并研究这种网络动态系统的稳定性。\\n\\n1985 年，G.E.Hinton 和 T.J.Sejnowski 借助统计物理学的概念和方法提出了一种随机神经网络模型——玻尔兹曼机(Boltzmann Machine)。一年后他们又改进了模型，提出了受限玻尔兹曼机(Restricted Boltzmann Machine)。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1986 年，Rumelhart，Hinton，Williams 提出了 BP(Back Propagation)算法[6]（多层感知器的误差反向传播算法）。到今天为止，这种多层感知器的误差反向传播算法还是非常基础的算法，凡是学神经网络的人，必然要学习 BP 算法。我们现在的深度网络模型基本上都是在这个算法的基础上发展出来的。使用 BP 算法的多层神经网络也称为 BP 神经网络（Back Propagation Neural network）。BP 神经网络主要指的是 20 世纪 80-90 年代使用 BP 算法的神经网络，虽然现在的深度学习也用 BP 算法，不过网络名称已经不叫 BP 神经网络了。早期的 BP 神经网络的神经元层数不能太多，一旦网络层数过多，就会使得网络无法训练，具体原因在后面的章节中会详细说明。Hopfield 神经网络，玻尔兹曼机以及受限玻尔兹曼机由于目前已经较少使用，所以本书后面章节不再详细介绍这三种网络。\\n\\n1.5.3 深度学习-2006 年至今', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1.5.3 深度学习-2006 年至今\\n\\n2006 年，多伦多大学的教授 Geoffrey Hinton 提出了深度学习。他在世界顶级学术期刊《Science》上发表了一篇论文《Reducing the dimensionality of data with neural networks》[7]，论文中提出了两个观点：①多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原始数据有更本质的代表性，这将大大便于分类和可视化问题；②对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。\\n\\nHinton 在论文中提出了一种新的网络结构深度置信网络（Deep Belief Net：DBN），这种网络使得训练深层的神经网络成为可能。深度置信网络由于目前已经较少使用，所以本书后面章节不再详细介绍这种网络。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='2012 年，Hinton 课题组为了证明深度学习的潜力，首次参加 ImageNet 图像识别比赛，通过 CNN 网络 AlexNet 一举夺得冠军。也正是由于该比赛，CNN 吸引了众多研究者的注意。\\n\\n\\n\\n2014 年，香港中文大学教授汤晓鸥领导的计算机视觉研究组开发了名为 DeepID 的深度学习模型， 在 LFW (Labeled Faces in the Wild，人脸识别使用非常广泛的测试基准)数据库上获得了 99.15%的识别率，人用肉眼在 LFW 上的识别率为 97.52%，深度学习在学术研究层面上已经超过了人用肉眼的识别。\\n\\n2016 年 3 月人工智能围棋比赛，由位于英国伦敦的谷歌（Google）旗下 DeepMind 公司的开发的 AlphaGo 战胜了世界围棋冠军、职业九段选手李世石，并以 4:1 的总比分获胜。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='2018 年 6 月，OpenAI 的研究人员开发了一种技术，可以在未标记的文本上训练 AI，可以大量减少人工标注的时间。几个月后谷歌推出了一个名为 BERT 的模型，该模型在学习了几百 万 个 句 子以后学会了 如 何 预测漏 掉 的 单 词 。 在 多 项 NLP (Natural Language Processing) 测试中，它的表现都接近人类。\\n\\n2020 年 6 月，OpenAI 发布了有史以来最大的 NLP 模型 GPT-3，GPT-3 模型参数达到了 1750 亿个参数，模型训练花费了上千万美元。GPT-3 训练方法很简单，但是却非常全能，可以完成填空，翻译，问答，阅读理解，数学计算，语法纠错等多项任务。随着 NLP 技术的发展，相信在将来 AI 可以逐渐理解我们的语言，跟我们进行顺畅的对话，甚至成为我们的保姆，老师或朋友。\\n\\n今天，人脸识别技术已经应用在了我们生活的方方面面，比如上下班打卡，飞机高铁出行，出门住酒店，刷脸支付等。我们已经离不开深度学习技术，而深度学习技术仍在快速发展中。\\n\\n1.6 深度学习领域重要人物', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1.6 深度学习领域重要人物\\n\\n深度学习领域有很多做出过卓越贡献的大师，下面简单介绍几位。前面的 3 位大师Geoffrey Hinton、Yann LeCun、Yoshua Bengio 江湖人称“深度学习三巨头”，为了表彰 3 位大师对于神经网络深度学习领域的贡献，2018 年计算机领域最高奖项图灵奖颁给了他们。\\n\\n1.Geoffrey Hinton\\n\\n英国出生的计算机学家和心理学家，以其在神经网络方面的贡献闻名。Hinton 是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。目前担任多伦多大学计算机科学系教授。2013 年 3 月加入 Google，领导 Google Brain 项目。Hinton 被人们称为“深度学习教父”，可以说是目前对深度学习领域影响最大的人。而且如今在深度学习领域活跃的大师，有很多都是他的弟子，可以说是桃李满天下。\\n\\n2.Yann LeCun', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='2.Yann LeCun\\n\\n法国出生的计算机科学家，他最著名的工作是光学字符识别和计算机视觉上使用卷积神经网络（CNN），他也被称为卷积网络之父。曾在多伦多大学跟随 Geoffrey Hinton 做博士后。1988 年加入贝尔实验室，在贝尔实验\\n\\n\\n\\n室工作期间开发了一套能够识别手写数字的卷积神经网络系统，并把它命名为 LeNet。这个系统能自动识别银行支票。\\n\\n2003 年去了纽约大学担任教授，现在是纽约大学终身教授。\\n\\n2013 年 12 月加入了 Facebook，成为 Facebook 人工智能实验室的第一任主任。\\n\\n3.Yoshua Bengio\\n\\n毕业于麦吉尔大学，在 MIT 和贝尔实验室做过博士后研究员，自 1993 年之后就在蒙特利尔大学任教。在预训练问题，自动编码器降噪等领域做出重大贡献。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='这“三巨头”中的前两人早已投身工业界，而 Bengio 仍留在学术界教书，他曾说过：“我留在学术圈是为全人类作贡献，而不是为某一公司赚钱”。他说这句话一定是因为他很有钱，开个玩笑。每个领域的发展不仅需要做前沿的研究，还需要不断培养新的新鲜血液加入到这个行业中，所以如果大学教授都去工作的话，上课教书的人就少了。所以 Bengio 能留在学术圈，对行业的发展也是一件好事。\\n\\n2017 年初 Bengio 选择加入微软成为战略顾问。他表示不希望有一家或者两家公司（他指的显然是 Google 和 Facebook）成为人工智能变革中的唯一大玩家，这对研究社区没有好处，对人类也没有好处。\\n\\n4.Andrew Ng（吴恩达）\\n\\nAndrew Ng 是美籍华人，曾经是斯坦福大学计算机科学系和电气工程系的副教授，斯坦福人工智能实验室主任。他还与 Daphne Koller 一起创建了在线教育平台 Coursera。\\n\\n2011 年，Andrew Ng 在 Google 创建了 Google Brain 项目，通过分布式集群计算机开发超大规模的人工神经网络。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='2014 年 5 月，Andrew Ng 加入百度，负责百度大脑计划，并担任百度公司首席科学家。\\n\\n2017 年 3 月，Andrew Ng 从百度离职，目前自己创业。\\n\\nLeCun 是 Hinton 的 博士生，另一位人工智能大师 Jordan 曾经申请过 Hinton 的博士生，Bengio 是 Jordan 的博士后，Andrew Ng 是 Jordan 的博士生，LeCun 与 Bengio 曾经是同事。这个圈子很小，大家都认识，这几位大师互相之间有着很深的渊源。\\n\\n曾经神经网络的圈子很小，基本上入了这个圈以后就没什么前途了。正是由于这个圈子里的这些大师前辈们的不懈努力，把神经网络算法不断优化，才有了今天的深度学习和今天人工智能的新局面。\\n\\n1.7 新一轮人工智能爆发的三要素\\n\\n这一轮人工智能大爆发的主要原因有 3 个，深度学习算法，大数据，以及高性能计算。深度学习算法 —— 之前人工智能领域的实际应用主要是使用传统的机器学习算法，虽然这些传统的机器学习算法在很多领域都取得了不错的效果，不过仍然有非常大的提升空间。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='深度学习出现后，计算机视觉，自然语言处理，语音识别等领域都取得了非常大的进步。大数据 —— 如果把人工智能比喻成一个火箭，那么这个火箭需要发射升空，它的燃料就是大数据。以前在实验室环境下很难收集到足够多的样本，现在的数据相对以前在数量、覆盖性和全面性方面都获得了大幅提升。一般来说深度学习模型想要获得好的效果，就需要把大量的数据放到模型中进行训练。\\n\\n高性能计算 —— 以前高性能计算大家用的是 CPU 集群，现在做深度学习都是用GPU(Graphics Processing Unit)或 TPU(Tensor Processing Unit)。想要使用大量的数据来训练复杂的深度学习模型那就必须要具备高性能计算能力。GPU 就是我们日常所说的显卡，平时主要用于打游戏。但是 GPU 不仅可以用于打游戏，还可以用来训练模型，性价比很高，买显卡的理由又多了一个。如果只是使用几个 CPU 来训练一个复杂模型可能会需要花费几周', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='甚至几个月的时间。把数百块 GPU 连接起来做成集群，用这些集群来训练模型，原来一个月才能训练出来的网络，可以加速到几个小时甚至几分钟就能训练完，可以大大减少模型训练时间。TPU 是谷歌专门为机器学习量身定做的处理器，执行每个操作所需的晶体管数量更少，效率更高。\\n\\n工欲善其事，必先利其器。下一章节我们将介绍如何搭建 python 开发环境，为我们后续的学习做准。', metadata={'source': 'test.docx'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs1 = text_splitter.split_documents(documents1)\n",
    "split_docs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0535d9e9-a87f-4ce5-8186-de59c8adb74b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:32:30.656318Z",
     "iopub.status.busy": "2023-12-01T02:32:30.656318Z",
     "iopub.status.idle": "2023-12-01T02:32:30.663227Z",
     "shell.execute_reply": "2023-12-01T02:32:30.663227Z",
     "shell.execute_reply.started": "2023-12-01T02:32:30.656318Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_docs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "795bef63-92c2-44cf-97cd-db3a0d71dc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:53:30.574994Z",
     "iopub.status.busy": "2023-12-01T02:53:30.573984Z",
     "iopub.status.idle": "2023-12-01T02:53:30.580128Z",
     "shell.execute_reply": "2023-12-01T02:53:30.579422Z",
     "shell.execute_reply.started": "2023-12-01T02:53:30.574994Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs1[0].metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678fa774-31d8-4dd9-a93e-acfe107033d0",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-01T02:32:40.325933Z",
     "iopub.status.busy": "2023-12-01T02:32:40.324928Z",
     "iopub.status.idle": "2023-12-01T02:32:40.414973Z",
     "shell.execute_reply": "2023-12-01T02:32:40.414014Z",
     "shell.execute_reply.started": "2023-12-01T02:32:40.325933Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1.5 神经网络深度学习发展史', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'Title'}),\n",
       " Document(page_content='神经网络的发展历史中有过三次热潮，分别发展在 20 世纪 40 年代到 60 年代，20 世纪80 年代到 90 年代，以及 2006 年至今。每一次神经网络的热潮都伴随着人工智能的兴起，人工智能和神经网络一直以来都有着非常密切的关系。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': 'a62c62fcb7f811a54ee5ab4be7c738db', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='1.5.1 神经网络诞生-20 世纪 40-60 年代', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': 'a62c62fcb7f811a54ee5ab4be7c738db', 'category_depth': 1, 'languages': ['zho'], 'page_number': 1, 'category': 'Title'}),\n",
       " Document(page_content='1943 年，神经病学家和神经元解剖学家 W.S.McCulloch 和数学家 W.A.Pitts 在生物物理学期刊发表文章提出神经元的数学描述和结构。并且证明了只要有足够的简单神经元，在这些神经元互相连接并同步运行的情况下，可以模拟任何计算函数，这种神经元的数学模型称为 MP 模型。该模型把神经元的动作描述为：', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'Title'}),\n",
       " Document(page_content='1.神经元的活动表现为兴奋或抑制的二值变化；', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'Title'}),\n",
       " Document(page_content='2.任何兴奋性突触输入激励后，使神经元兴奋；', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'Title'}),\n",
       " Document(page_content='3.任何抑制性突触有输入激励后，使神经元抑制；', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'Title'}),\n",
       " Document(page_content='4.突触的值不随时间改变；5.突触从感知输入到传送出一个输出脉冲的延时时间是 0.5ms。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'Title'}),\n",
       " Document(page_content='尽管现在看来 M-P 模型过于简单，并且观点也不是完全正确，不过这个模型被认为是第一个仿生学的神经网络模型，他们提出的很多观点一直沿用至今，比如说他们认为神经元有两种状态，要不就是兴奋，要不就是抑制。这跟后面要提到的单层感知器非常类似，单层感知器的输出要不就是 0 要不就是 1。他们最重要的贡献就是开创了神经网络这个研究方向，为今天神经网络的发展奠定了基础。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': '0d09857e920faf71ec2d3446cdb8a606', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='1949 年 ， 另 一 位 心 理 学 家 Donald Olding Hebb 在 他 的一本 名 为《The organization of behavior: A neuropsychological theory》[4]的书提出了 Hebb 算法。他也是首先提出“连接主义”（connectionism）这一名词的人之一，这个名词的含义是大脑的活动是靠脑细胞的组合连接实现的。Hebb 认为，如果源和目的神经元均被激活兴奋时，它们之间突触的连接强度将会增强。他指出在神经网络中，信息存储在连接权值中。并提出假设神经元 A 到神经元 B 连接权与从 B 到 A 的连接权是相同的。他这里提到的这个权值的思想也被应用到了我们目前所使用的神经网络中，我们通过调节神经元之间的连接权值来得到不同的神经网络模型，实现不同的应用。虽然这些理论在今天看来是理所当然的，不过在当时看来这是一种全新的想法，算得上是开创性的理论。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': '0d09857e920faf71ec2d3446cdb8a606', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'NarrativeText'}),\n",
       " Document(page_content='1958 年，计算机学家 Frank Rosenblatt 提出了一种神经网络结构，称为感知器(Perceptron)。他提出的这个感知器可能是世界上第一个真正意义上的人工神经网络。感知器提出之后在 60 年代就掀起了神经网络研究的第一次热潮。很多人都认为只要使用成千上万的神经元，他们就能解决一切问题。现在看来可能会让人感觉 too young too naive，不过感知器在当时确实是影响非凡。这股感知器热潮持续了 10 年，直到 1969 年，人工智能的创始人之一的 M.Minsky 和S.Papert 出版了一本名为《感知器》[5]的书，书中指出简单神经网络只能运用于线性问题的求解，能够求解非线性问题的网络应具有隐层，而从理论上还不能证明将感知器模型扩展到多层网络是有意义的。由于 Minsky 在学术界的地位和影响，其悲观论点极大地影响了当时的人工神经网络研究，为刚刚燃起希望之火的人工神经网络泼了一大盘冷水。这本书出版不不久之后，几乎所有为神经网络提供的研究基金都枯竭了，没有人愿意把钱浪费在没有意义的事情上。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': '0d09857e920faf71ec2d3446cdb8a606', 'category_depth': 0, 'languages': ['zho'], 'page_number': 1, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='', metadata={'source': 'test.docx', 'filename': 'test.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['zho'], 'category': 'PageBreak'}),\n",
       " Document(page_content='1.5.2 神经网络复兴-20 世纪 80-90 年代', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 1, 'languages': ['zho'], 'page_number': 2, 'category': 'Title'}),\n",
       " Document(page_content='1982 年，美国加州理工学院的优秀物理学家 John J.Hopfield 博士提出了 Hopfield 神经网络。Hopfield 神经网络引用了物理力学的分析方法，把网络作为一种动态系统并研究这种网络动态系统的稳定性。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 2, 'category': 'Title'}),\n",
       " Document(page_content='1985 年，G.E.Hinton 和 T.J.Sejnowski 借助统计物理学的概念和方法提出了一种随机神经网络模型——玻尔兹曼机(Boltzmann Machine)。一年后他们又改进了模型，提出了受限玻尔兹曼机(Restricted Boltzmann Machine)。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 2, 'category': 'Title'}),\n",
       " Document(page_content='1986 年，Rumelhart，Hinton，Williams 提出了 BP(Back Propagation)算法[6]（多层感知器的误差反向传播算法）。到今天为止，这种多层感知器的误差反向传播算法还是非常基础的算法，凡是学神经网络的人，必然要学习 BP 算法。我们现在的深度网络模型基本上都是在这个算法的基础上发展出来的。使用 BP 算法的多层神经网络也称为 BP 神经网络（Back Propagation Neural network）。BP 神经网络主要指的是 20 世纪 80-90 年代使用 BP 算法的神经网络，虽然现在的深度学习也用 BP 算法，不过网络名称已经不叫 BP 神经网络了。早期的 BP 神经网络的神经元层数不能太多，一旦网络层数过多，就会使得网络无法训练，具体原因在后面的章节中会详细说明。Hopfield 神经网络，玻尔兹曼机以及受限玻尔兹曼机由于目前已经较少使用，所以本书后面章节不再详细介绍这三种网络。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': 'c42785d8915fc72467906080c803a641', 'category_depth': 0, 'languages': ['zho'], 'page_number': 2, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='1.5.3 深度学习-2006 年至今', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': 'c42785d8915fc72467906080c803a641', 'category_depth': 1, 'languages': ['zho'], 'page_number': 2, 'category': 'Title'}),\n",
       " Document(page_content='2006 年，多伦多大学的教授 Geoffrey Hinton 提出了深度学习。他在世界顶级学术期刊《Science》上发表了一篇论文《Reducing the dimensionality of data with neural networks》[7]，论文中提出了两个观点：①多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原始数据有更本质的代表性，这将大大便于分类和可视化问题；②对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': 'c49e3511dfd47f65430c9d83a40c7c2c', 'category_depth': 0, 'languages': ['zho'], 'page_number': 2, 'category': 'NarrativeText'}),\n",
       " Document(page_content='Hinton 在论文中提出了一种新的网络结构深度置信网络（Deep Belief Net：DBN），这种网络使得训练深层的神经网络成为可能。深度置信网络由于目前已经较少使用，所以本书后面章节不再详细介绍这种网络。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 2, 'category': 'Title'}),\n",
       " Document(page_content='2012 年，Hinton 课题组为了证明深度学习的潜力，首次参加 ImageNet 图像识别比赛，通过 CNN 网络 AlexNet 一举夺得冠军。也正是由于该比赛，CNN 吸引了众多研究者的注意。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 2, 'category': 'Title'}),\n",
       " Document(page_content='', metadata={'source': 'test.docx', 'filename': 'test.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['zho'], 'category': 'PageBreak'}),\n",
       " Document(page_content='2014 年，香港中文大学教授汤晓鸥领导的计算机视觉研究组开发了名为 DeepID 的深度学习模型， 在 LFW (Labeled Faces in the Wild，人脸识别使用非常广泛的测试基准)数据库上获得了 99.15%的识别率，人用肉眼在 LFW 上的识别率为 97.52%，深度学习在学术研究层面上已经超过了人用肉眼的识别。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'NarrativeText'}),\n",
       " Document(page_content='2016 年 3 月人工智能围棋比赛，由位于英国伦敦的谷歌（Google）旗下 DeepMind 公司的开发的 AlphaGo 战胜了世界围棋冠军、职业九段选手李世石，并以 4:1 的总比分获胜。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'NarrativeText'}),\n",
       " Document(page_content='2018 年 6 月，OpenAI 的研究人员开发了一种技术，可以在未标记的文本上训练 AI，可以大量减少人工标注的时间。几个月后谷歌推出了一个名为 BERT 的模型，该模型在学习了几百 万 个 句 子以后学会了 如 何 预测漏 掉 的 单 词 。 在 多 项 NLP (Natural Language Processing) 测试中，它的表现都接近人类。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='2020 年 6 月，OpenAI 发布了有史以来最大的 NLP 模型 GPT-3，GPT-3 模型参数达到了 1750 亿个参数，模型训练花费了上千万美元。GPT-3 训练方法很简单，但是却非常全能，可以完成填空，翻译，问答，阅读理解，数学计算，语法纠错等多项任务。随着 NLP 技术的发展，相信在将来 AI 可以逐渐理解我们的语言，跟我们进行顺畅的对话，甚至成为我们的保姆，老师或朋友。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='今天，人脸识别技术已经应用在了我们生活的方方面面，比如上下班打卡，飞机高铁出行，出门住酒店，刷脸支付等。我们已经离不开深度学习技术，而深度学习技术仍在快速发展中。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'Title'}),\n",
       " Document(page_content='1.6 深度学习领域重要人物', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'Title'}),\n",
       " Document(page_content='深度学习领域有很多做出过卓越贡献的大师，下面简单介绍几位。前面的 3 位大师Geoffrey Hinton、Yann LeCun、Yoshua Bengio 江湖人称“深度学习三巨头”，为了表彰 3 位大师对于神经网络深度学习领域的贡献，2018 年计算机领域最高奖项图灵奖颁给了他们。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': '1ea8d89541ee1b30e7f9fdf85186d880', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'NarrativeText'}),\n",
       " Document(page_content='1.Geoffrey Hinton', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'Title'}),\n",
       " Document(page_content='英国出生的计算机学家和心理学家，以其在神经网络方面的贡献闻名。Hinton 是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。目前担任多伦多大学计算机科学系教授。2013 年 3 月加入 Google，领导 Google Brain 项目。Hinton 被人们称为“深度学习教父”，可以说是目前对深度学习领域影响最大的人。而且如今在深度学习领域活跃的大师，有很多都是他的弟子，可以说是桃李满天下。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': '46a512c836b7b405b7f497caf738b3aa', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'NarrativeText'}),\n",
       " Document(page_content='2.Yann LeCun', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'Title'}),\n",
       " Document(page_content='法国出生的计算机科学家，他最著名的工作是光学字符识别和计算机视觉上使用卷积神经网络（CNN），他也被称为卷积网络之父。曾在多伦多大学跟随 Geoffrey Hinton 做博士后。1988 年加入贝尔实验室，在贝尔实验', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 3, 'category': 'Title'}),\n",
       " Document(page_content='', metadata={'source': 'test.docx', 'filename': 'test.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['zho'], 'category': 'PageBreak'}),\n",
       " Document(page_content='室工作期间开发了一套能够识别手写数字的卷积神经网络系统，并把它命名为 LeNet。这个系统能自动识别银行支票。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='2003 年去了纽约大学担任教授，现在是纽约大学终身教授。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='2013 年 12 月加入了 Facebook，成为 Facebook 人工智能实验室的第一任主任。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='3.Yoshua Bengio', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='毕业于麦吉尔大学，在 MIT 和贝尔实验室做过博士后研究员，自 1993 年之后就在蒙特利尔大学任教。在预训练问题，自动编码器降噪等领域做出重大贡献。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': '784033d2369b22489cd863a949000ddd', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='这“三巨头”中的前两人早已投身工业界，而 Bengio 仍留在学术界教书，他曾说过：“我留在学术圈是为全人类作贡献，而不是为某一公司赚钱”。他说这句话一定是因为他很有钱，开个玩笑。每个领域的发展不仅需要做前沿的研究，还需要不断培养新的新鲜血液加入到这个行业中，所以如果大学教授都去工作的话，上课教书的人就少了。所以 Bengio 能留在学术圈，对行业的发展也是一件好事。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='2017 年初 Bengio 选择加入微软成为战略顾问。他表示不希望有一家或者两家公司（他指的显然是 Google 和 Facebook）成为人工智能变革中的唯一大玩家，这对研究社区没有好处，对人类也没有好处。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='4.Andrew Ng（吴恩达）', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='Andrew Ng 是美籍华人，曾经是斯坦福大学计算机科学系和电气工程系的副教授，斯坦福人工智能实验室主任。他还与 Daphne Koller 一起创建了在线教育平台 Coursera。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='2011 年，Andrew Ng 在 Google 创建了 Google Brain 项目，通过分布式集群计算机开发超大规模的人工神经网络。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='2014 年 5 月，Andrew Ng 加入百度，负责百度大脑计划，并担任百度公司首席科学家。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='2017 年 3 月，Andrew Ng 从百度离职，目前自己创业。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='LeCun 是 Hinton 的 博士生，另一位人工智能大师 Jordan 曾经申请过 Hinton 的博士生，Bengio 是 Jordan 的博士后，Andrew Ng 是 Jordan 的博士生，LeCun 与 Bengio 曾经是同事。这个圈子很小，大家都认识，这几位大师互相之间有着很深的渊源。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': 'd79824c130cdff2674ea66dde40466b0', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='曾经神经网络的圈子很小，基本上入了这个圈以后就没什么前途了。正是由于这个圈子里的这些大师前辈们的不懈努力，把神经网络算法不断优化，才有了今天的深度学习和今天人工智能的新局面。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='1.7 新一轮人工智能爆发的三要素', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='这一轮人工智能大爆发的主要原因有 3 个，深度学习算法，大数据，以及高性能计算。深度学习算法 —— 之前人工智能领域的实际应用主要是使用传统的机器学习算法，虽然这些传统的机器学习算法在很多领域都取得了不错的效果，不过仍然有非常大的提升空间。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 4, 'category': 'Title'}),\n",
       " Document(page_content='', metadata={'source': 'test.docx', 'filename': 'test.docx', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'languages': ['zho'], 'category': 'PageBreak'}),\n",
       " Document(page_content='深度学习出现后，计算机视觉，自然语言处理，语音识别等领域都取得了非常大的进步。大数据 —— 如果把人工智能比喻成一个火箭，那么这个火箭需要发射升空，它的燃料就是大数据。以前在实验室环境下很难收集到足够多的样本，现在的数据相对以前在数量、覆盖性和全面性方面都获得了大幅提升。一般来说深度学习模型想要获得好的效果，就需要把大量的数据放到模型中进行训练。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 5, 'category': 'Title'}),\n",
       " Document(page_content='高性能计算 —— 以前高性能计算大家用的是 CPU 集群，现在做深度学习都是用GPU(Graphics Processing Unit)或 TPU(Tensor Processing Unit)。想要使用大量的数据来训练复杂的深度学习模型那就必须要具备高性能计算能力。GPU 就是我们日常所说的显卡，平时主要用于打游戏。但是 GPU 不仅可以用于打游戏，还可以用来训练模型，性价比很高，买显卡的理由又多了一个。如果只是使用几个 CPU 来训练一个复杂模型可能会需要花费几周', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': '3957a7417a6fb895eab2e264eaaf2b38', 'category_depth': 0, 'languages': ['zho'], 'page_number': 5, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='甚至几个月的时间。把数百块 GPU 连接起来做成集群，用这些集群来训练模型，原来一个月才能训练出来的网络，可以加速到几个小时甚至几分钟就能训练完，可以大大减少模型训练时间。TPU 是谷歌专门为机器学习量身定做的处理器，执行每个操作所需的晶体管数量更少，效率更高。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'parent_id': '3957a7417a6fb895eab2e264eaaf2b38', 'category_depth': 0, 'languages': ['zho'], 'page_number': 5, 'category': 'UncategorizedText'}),\n",
       " Document(page_content='工欲善其事，必先利其器。下一章节我们将介绍如何搭建 python 开发环境，为我们后续的学习做准。', metadata={'source': 'test.docx', 'filename': 'test.docx', 'last_modified': '2023-12-01T10:00:44', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category_depth': 0, 'languages': ['zho'], 'page_number': 5, 'category': 'Title'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "loader2 = UnstructuredWordDocumentLoader(\"test.docx\", mode=\"elements\")\n",
    "documents2 = loader2.load()\n",
    "documents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "475262cc-9e4a-4b34-88dd-1bc296a5c0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:51:02.843817Z",
     "iopub.status.busy": "2023-12-01T02:51:02.842819Z",
     "iopub.status.idle": "2023-12-01T02:51:02.849664Z",
     "shell.execute_reply": "2023-12-01T02:51:02.849460Z",
     "shell.execute_reply.started": "2023-12-01T02:51:02.843817Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f5d5c45-e544-47cb-b49b-2c82a0f9bce1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:51:07.794431Z",
     "iopub.status.busy": "2023-12-01T02:51:07.794325Z",
     "iopub.status.idle": "2023-12-01T02:51:07.802875Z",
     "shell.execute_reply": "2023-12-01T02:51:07.802169Z",
     "shell.execute_reply.started": "2023-12-01T02:51:07.794431Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'filename', 'last_modified', 'filetype', 'category_depth', 'languages', 'page_number', 'category'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents2[0].metadata.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d49978-df9f-4788-90e4-164ab12aaf45",
   "metadata": {},
   "source": [
    "# Docx2txtLoader\n",
    "\n",
    "Docx2txtLoader不支持使用mode=\"elements\"参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce687a3-dbbd-43e2-b1e0-6b933479d248",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-01T02:34:02.609132Z",
     "iopub.status.busy": "2023-12-01T02:34:02.609132Z",
     "iopub.status.idle": "2023-12-01T02:34:02.628519Z",
     "shell.execute_reply": "2023-12-01T02:34:02.627508Z",
     "shell.execute_reply.started": "2023-12-01T02:34:02.609132Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1.5 神经网络深度学习发展史\\n\\n神经网络的发展历史中有过三次热潮，分别发展在 20 世纪 40 年代到 60 年代，20 世纪80 年代到 90 年代，以及 2006 年至今。每一次神经网络的热潮都伴随着人工智能的兴起，人工智能和神经网络一直以来都有着非常密切的关系。\\n\\n\\n\\n1.5.1 神经网络诞生-20 世纪 40-60 年代\\n\\n1943 年，神经病学家和神经元解剖学家 W.S.McCulloch 和数学家 W.A.Pitts 在生物物理学期刊发表文章提出神经元的数学描述和结构。并且证明了只要有足够的简单神经元，在这些神经元互相连接并同步运行的情况下，可以模拟任何计算函数，这种神经元的数学模型称为 MP 模型。该模型把神经元的动作描述为：\\n\\n1.神经元的活动表现为兴奋或抑制的二值变化；\\n\\n2.任何兴奋性突触输入激励后，使神经元兴奋；\\n\\n3.任何抑制性突触有输入激励后，使神经元抑制；\\n\\n4.突触的值不随时间改变；5.突触从感知输入到传送出一个输出脉冲的延时时间是 0.5ms。\\n\\n\\n\\n尽管现在看来 M-P 模型过于简单，并且观点也不是完全正确，不过这个模型被认为是第一个仿生学的神经网络模型，他们提出的很多观点一直沿用至今，比如说他们认为神经元有两种状态，要不就是兴奋，要不就是抑制。这跟后面要提到的单层感知器非常类似，单层感知器的输出要不就是 0 要不就是 1。他们最重要的贡献就是开创了神经网络这个研究方向，为今天神经网络的发展奠定了基础。\\n\\n\\n\\n1949 年 ， 另 一 位 心 理 学 家 Donald Olding Hebb 在 他 的一本 名 为《The organization of behavior: A neuropsychological theory》[4]的书提出了 Hebb 算法。他也是首先提出“连接主义”（connectionism）这一名词的人之一，这个名词的含义是大脑的活动是靠脑细胞的组合连接实现的。Hebb 认为，如果源和目的神经元均被激活兴奋时，它们之间突触的连接强度将会增强。他指出在神经网络中，信息存储在连接权值中。并提出假设神经元 A 到神经元 B 连接权与从 B 到 A 的连接权是相同的。他这里提到的这个权值的思想也被应用到了我们目前所使用的神经网络中，我们通过调节神经元之间的连接权值来得到不同的神经网络模型，实现不同的应用。虽然这些理论在今天看来是理所当然的，不过在当时看来这是一种全新的想法，算得上是开创性的理论。\\n\\n\\n\\n1958 年，计算机学家 Frank Rosenblatt 提出了一种神经网络结构，称为感知器(Perceptron)。他提出的这个感知器可能是世界上第一个真正意义上的人工神经网络。感知器提出之后在 60 年代就掀起了神经网络研究的第一次热潮。很多人都认为只要使用成千上万的神经元，他们就能解决一切问题。现在看来可能会让人感觉 too young too naive，不过感知器在当时确实是影响非凡。这股感知器热潮持续了 10 年，直到 1969 年，人工智能的创始人之一的 M.Minsky 和S.Papert 出版了一本名为《感知器》[5]的书，书中指出简单神经网络只能运用于线性问题的求解，能够求解非线性问题的网络应具有隐层，而从理论上还不能证明将感知器模型扩展到多层网络是有意义的。由于 Minsky 在学术界的地位和影响，其悲观论点极大地影响了当时的人工神经网络研究，为刚刚燃起希望之火的人工神经网络泼了一大盘冷水。这本书出版不不久之后，几乎所有为神经网络提供的研究基金都枯竭了，没有人愿意把钱浪费在没有意义的事情上。\\n\\n\\n\\n1.5.2 神经网络复兴-20 世纪 80-90 年代\\n\\n\\n\\n1982 年，美国加州理工学院的优秀物理学家 John J.Hopfield 博士提出了 Hopfield 神经网络。Hopfield 神经网络引用了物理力学的分析方法，把网络作为一种动态系统并研究这种网络动态系统的稳定性。\\n\\n\\n\\n1985 年，G.E.Hinton 和 T.J.Sejnowski 借助统计物理学的概念和方法提出了一种随机神经网络模型——玻尔兹曼机(Boltzmann Machine)。一年后他们又改进了模型，提出了受限玻尔兹曼机(Restricted Boltzmann Machine)。\\n\\n\\n\\n1986 年，Rumelhart，Hinton，Williams 提出了 BP(Back Propagation)算法[6]（多层感知器的误差反向传播算法）。到今天为止，这种多层感知器的误差反向传播算法还是非常基础的算法，凡是学神经网络的人，必然要学习 BP 算法。我们现在的深度网络模型基本上都是在这个算法的基础上发展出来的。使用 BP 算法的多层神经网络也称为 BP 神经网络（Back Propagation Neural network）。BP 神经网络主要指的是 20 世纪 80-90 年代使用 BP 算法的神经网络，虽然现在的深度学习也用 BP 算法，不过网络名称已经不叫 BP 神经网络了。早期的 BP 神经网络的神经元层数不能太多，一旦网络层数过多，就会使得网络无法训练，具体原因在后面的章节中会详细说明。Hopfield 神经网络，玻尔兹曼机以及受限玻尔兹曼机由于目前已经较少使用，所以本书后面章节不再详细介绍这三种网络。\\n\\n\\n\\n1.5.3 深度学习-2006 年至今\\n\\n\\n\\n2006 年，多伦多大学的教授 Geoffrey Hinton 提出了深度学习。他在世界顶级学术期刊《Science》上发表了一篇论文《Reducing the dimensionality of data with neural networks》[7]，论文中提出了两个观点：①多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原始数据有更本质的代表性，这将大大便于分类和可视化问题；②对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。\\n\\n\\n\\nHinton 在论文中提出了一种新的网络结构深度置信网络（Deep Belief Net：DBN），这种网络使得训练深层的神经网络成为可能。深度置信网络由于目前已经较少使用，所以本书后面章节不再详细介绍这种网络。\\n\\n\\n\\n2012 年，Hinton 课题组为了证明深度学习的潜力，首次参加 ImageNet 图像识别比赛，通过 CNN 网络 AlexNet 一举夺得冠军。也正是由于该比赛，CNN 吸引了众多研究者的注意。\\n\\n\\n\\n2014 年，香港中文大学教授汤晓鸥领导的计算机视觉研究组开发了名为 DeepID 的深度学习模型， 在 LFW (Labeled Faces in the Wild，人脸识别使用非常广泛的测试基准)数据库上获得了 99.15%的识别率，人用肉眼在 LFW 上的识别率为 97.52%，深度学习在学术研究层面上已经超过了人用肉眼的识别。\\n\\n\\n\\n2016 年 3 月人工智能围棋比赛，由位于英国伦敦的谷歌（Google）旗下 DeepMind 公司的开发的 AlphaGo 战胜了世界围棋冠军、职业九段选手李世石，并以 4:1 的总比分获胜。\\n\\n\\n\\n2018 年 6 月，OpenAI 的研究人员开发了一种技术，可以在未标记的文本上训练 AI，可以大量减少人工标注的时间。几个月后谷歌推出了一个名为 BERT 的模型，该模型在学习了几百 万 个 句 子以后学会了 如 何 预测漏 掉 的 单 词 。 在 多 项 NLP (Natural Language Processing) 测试中，它的表现都接近人类。\\n\\n\\n\\n2020 年 6 月，OpenAI 发布了有史以来最大的 NLP 模型 GPT-3，GPT-3 模型参数达到了 1750 亿个参数，模型训练花费了上千万美元。GPT-3 训练方法很简单，但是却非常全能，可以完成填空，翻译，问答，阅读理解，数学计算，语法纠错等多项任务。随着 NLP 技术的发展，相信在将来 AI 可以逐渐理解我们的语言，跟我们进行顺畅的对话，甚至成为我们的保姆，老师或朋友。\\n\\n\\n\\n今天，人脸识别技术已经应用在了我们生活的方方面面，比如上下班打卡，飞机高铁出行，出门住酒店，刷脸支付等。我们已经离不开深度学习技术，而深度学习技术仍在快速发展中。\\n\\n\\n\\n1.6 深度学习领域重要人物\\n\\n\\n\\n深度学习领域有很多做出过卓越贡献的大师，下面简单介绍几位。前面的 3 位大师Geoffrey Hinton、Yann LeCun、Yoshua Bengio 江湖人称“深度学习三巨头”，为了表彰 3 位大师对于神经网络深度学习领域的贡献，2018 年计算机领域最高奖项图灵奖颁给了他们。\\n\\n\\n\\n1.Geoffrey Hinton\\n\\n\\n\\n英国出生的计算机学家和心理学家，以其在神经网络方面的贡献闻名。Hinton 是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。目前担任多伦多大学计算机科学系教授。2013 年 3 月加入 Google，领导 Google Brain 项目。Hinton 被人们称为“深度学习教父”，可以说是目前对深度学习领域影响最大的人。而且如今在深度学习领域活跃的大师，有很多都是他的弟子，可以说是桃李满天下。\\n\\n\\n\\n2.Yann LeCun\\n\\n\\n\\n法国出生的计算机科学家，他最著名的工作是光学字符识别和计算机视觉上使用卷积神经网络（CNN），他也被称为卷积网络之父。曾在多伦多大学跟随 Geoffrey Hinton 做博士后。1988 年加入贝尔实验室，在贝尔实验\\n\\n室工作期间开发了一套能够识别手写数字的卷积神经网络系统，并把它命名为 LeNet。这个系统能自动识别银行支票。\\n\\n2003 年去了纽约大学担任教授，现在是纽约大学终身教授。\\n\\n2013 年 12 月加入了 Facebook，成为 Facebook 人工智能实验室的第一任主任。\\n\\n\\n\\n3.Yoshua Bengio\\n\\n\\n\\n毕业于麦吉尔大学，在 MIT 和贝尔实验室做过博士后研究员，自 1993 年之后就在蒙特利尔大学任教。在预训练问题，自动编码器降噪等领域做出重大贡献。\\n\\n这“三巨头”中的前两人早已投身工业界，而 Bengio 仍留在学术界教书，他曾说过：“我留在学术圈是为全人类作贡献，而不是为某一公司赚钱”。他说这句话一定是因为他很有钱，开个玩笑。每个领域的发展不仅需要做前沿的研究，还需要不断培养新的新鲜血液加入到这个行业中，所以如果大学教授都去工作的话，上课教书的人就少了。所以 Bengio 能留在学术圈，对行业的发展也是一件好事。\\n\\n2017 年初 Bengio 选择加入微软成为战略顾问。他表示不希望有一家或者两家公司（他指的显然是 Google 和 Facebook）成为人工智能变革中的唯一大玩家，这对研究社区没有好处，对人类也没有好处。\\n\\n\\n\\n4.Andrew Ng（吴恩达）\\n\\n\\n\\nAndrew Ng 是美籍华人，曾经是斯坦福大学计算机科学系和电气工程系的副教授，斯坦福人工智能实验室主任。他还与 Daphne Koller 一起创建了在线教育平台 Coursera。\\n\\n2011 年，Andrew Ng 在 Google 创建了 Google Brain 项目，通过分布式集群计算机开发超大规模的人工神经网络。\\n\\n2014 年 5 月，Andrew Ng 加入百度，负责百度大脑计划，并担任百度公司首席科学家。\\n\\n2017 年 3 月，Andrew Ng 从百度离职，目前自己创业。\\n\\nLeCun 是 Hinton 的 博士生，另一位人工智能大师 Jordan 曾经申请过 Hinton 的博士生，Bengio 是 Jordan 的博士后，Andrew Ng 是 Jordan 的博士生，LeCun 与 Bengio 曾经是同事。这个圈子很小，大家都认识，这几位大师互相之间有着很深的渊源。\\n\\n\\n\\n曾经神经网络的圈子很小，基本上入了这个圈以后就没什么前途了。正是由于这个圈子里的这些大师前辈们的不懈努力，把神经网络算法不断优化，才有了今天的深度学习和今天人工智能的新局面。\\n\\n\\n\\n1.7 新一轮人工智能爆发的三要素\\n\\n\\n\\n这一轮人工智能大爆发的主要原因有 3 个，深度学习算法，大数据，以及高性能计算。深度学习算法 —— 之前人工智能领域的实际应用主要是使用传统的机器学习算法，虽然这些传统的机器学习算法在很多领域都取得了不错的效果，不过仍然有非常大的提升空间。\\n\\n\\n\\n深度学习出现后，计算机视觉，自然语言处理，语音识别等领域都取得了非常大的进步。大数据 —— 如果把人工智能比喻成一个火箭，那么这个火箭需要发射升空，它的燃料就是大数据。以前在实验室环境下很难收集到足够多的样本，现在的数据相对以前在数量、覆盖性和全面性方面都获得了大幅提升。一般来说深度学习模型想要获得好的效果，就需要把大量的数据放到模型中进行训练。\\n\\n\\n\\n高性能计算 —— 以前高性能计算大家用的是 CPU 集群，现在做深度学习都是用GPU(Graphics Processing Unit)或 TPU(Tensor Processing Unit)。想要使用大量的数据来训练复杂的深度学习模型那就必须要具备高性能计算能力。GPU 就是我们日常所说的显卡，平时主要用于打游戏。但是 GPU 不仅可以用于打游戏，还可以用来训练模型，性价比很高，买显卡的理由又多了一个。如果只是使用几个 CPU 来训练一个复杂模型可能会需要花费几周\\n\\n甚至几个月的时间。把数百块 GPU 连接起来做成集群，用这些集群来训练模型，原来一个月才能训练出来的网络，可以加速到几个小时甚至几分钟就能训练完，可以大大减少模型训练时间。TPU 是谷歌专门为机器学习量身定做的处理器，执行每个操作所需的晶体管数量更少，效率更高。\\n\\n\\n\\n工欲善其事，必先利其器。下一章节我们将介绍如何搭建 python 开发环境，为我们后续的学习做准。', metadata={'source': 'test.docx'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader3 = Docx2txtLoader(\"test.docx\")\n",
    "documents3 = loader3.load()\n",
    "documents3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551fd3d2-81b7-46c9-aafa-c6cd8ff9ce25",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-01T02:35:29.404120Z",
     "iopub.status.busy": "2023-12-01T02:35:29.404120Z",
     "iopub.status.idle": "2023-12-01T02:35:29.414931Z",
     "shell.execute_reply": "2023-12-01T02:35:29.414641Z",
     "shell.execute_reply.started": "2023-12-01T02:35:29.404120Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1.5 神经网络深度学习发展史\\n\\n神经网络的发展历史中有过三次热潮，分别发展在 20 世纪 40 年代到 60 年代，20 世纪80 年代到 90 年代，以及 2006 年至今。每一次神经网络的热潮都伴随着人工智能的兴起，人工智能和神经网络一直以来都有着非常密切的关系。\\n\\n\\n\\n1.5.1 神经网络诞生-20 世纪 40-60 年代\\n\\n1943 年，神经病学家和神经元解剖学家 W.S.McCulloch 和数学家 W.A.Pitts 在生物物理学期刊发表文章提出神经元的数学描述和结构。并且证明了只要有足够的简单神经元，在这些神经元互相连接并同步运行的情况下，可以模拟任何计算函数，这种神经元的数学模型称为 MP 模型。该模型把神经元的动作描述为：\\n\\n1.神经元的活动表现为兴奋或抑制的二值变化；\\n\\n2.任何兴奋性突触输入激励后，使神经元兴奋；\\n\\n3.任何抑制性突触有输入激励后，使神经元抑制；\\n\\n4.突触的值不随时间改变；5.突触从感知输入到传送出一个输出脉冲的延时时间是 0.5ms。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='4.突触的值不随时间改变；5.突触从感知输入到传送出一个输出脉冲的延时时间是 0.5ms。\\n\\n\\n\\n尽管现在看来 M-P 模型过于简单，并且观点也不是完全正确，不过这个模型被认为是第一个仿生学的神经网络模型，他们提出的很多观点一直沿用至今，比如说他们认为神经元有两种状态，要不就是兴奋，要不就是抑制。这跟后面要提到的单层感知器非常类似，单层感知器的输出要不就是 0 要不就是 1。他们最重要的贡献就是开创了神经网络这个研究方向，为今天神经网络的发展奠定了基础。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1949 年 ， 另 一 位 心 理 学 家 Donald Olding Hebb 在 他 的一本 名 为《The organization of behavior: A neuropsychological theory》[4]的书提出了 Hebb 算法。他也是首先提出“连接主义”（connectionism）这一名词的人之一，这个名词的含义是大脑的活动是靠脑细胞的组合连接实现的。Hebb 认为，如果源和目的神经元均被激活兴奋时，它们之间突触的连接强度将会增强。他指出在神经网络中，信息存储在连接权值中。并提出假设神经元 A 到神经元 B 连接权与从 B 到 A 的连接权是相同的。他这里提到的这个权值的思想也被应用到了我们目前所使用的神经网络中，我们通过调节神经元之间的连接权值来得到不同的神经网络模型，实现不同的应用。虽然这些理论在今天看来是理所当然的，不过在当时看来这是一种全新的想法，算得上是开创性的理论。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1958 年，计算机学家 Frank Rosenblatt 提出了一种神经网络结构，称为感知器(Perceptron)。他提出的这个感知器可能是世界上第一个真正意义上的人工神经网络。感知器提出之后在 60 年代就掀起了神经网络研究的第一次热潮。很多人都认为只要使用成千上万的神经元，他们就能解决一切问题。现在看来可能会让人感觉 too young too naive，不过感知器在当时确实是影响非凡。这股感知器热潮持续了 10 年，直到 1969 年，人工智能的创始人之一的 M.Minsky 和S.Papert 出版了一本名为《感知器》[5]的书，书中指出简单神经网络只能运用于线性问题的求解，能够求解非线性问题的网络应具有隐层，而从理论上还不能证明将感知器模型扩展到多层网络是有意义的。由于 Minsky 在学术界的地位和影响，其悲观论点极大地影响了当时的人工神经网络研究，为刚刚燃起希望之火的人工神经网络泼了一大盘冷水。这本书出版不不久之后，几乎所有为神经网络提供的研究基金都枯竭了，没有人愿意把钱浪费在没有意义的事情上。\\n\\n\\n\\n1.5.2 神经网络复兴-20 世纪 80-90 年代', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1.5.2 神经网络复兴-20 世纪 80-90 年代\\n\\n\\n\\n1982 年，美国加州理工学院的优秀物理学家 John J.Hopfield 博士提出了 Hopfield 神经网络。Hopfield 神经网络引用了物理力学的分析方法，把网络作为一种动态系统并研究这种网络动态系统的稳定性。\\n\\n\\n\\n1985 年，G.E.Hinton 和 T.J.Sejnowski 借助统计物理学的概念和方法提出了一种随机神经网络模型——玻尔兹曼机(Boltzmann Machine)。一年后他们又改进了模型，提出了受限玻尔兹曼机(Restricted Boltzmann Machine)。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1986 年，Rumelhart，Hinton，Williams 提出了 BP(Back Propagation)算法[6]（多层感知器的误差反向传播算法）。到今天为止，这种多层感知器的误差反向传播算法还是非常基础的算法，凡是学神经网络的人，必然要学习 BP 算法。我们现在的深度网络模型基本上都是在这个算法的基础上发展出来的。使用 BP 算法的多层神经网络也称为 BP 神经网络（Back Propagation Neural network）。BP 神经网络主要指的是 20 世纪 80-90 年代使用 BP 算法的神经网络，虽然现在的深度学习也用 BP 算法，不过网络名称已经不叫 BP 神经网络了。早期的 BP 神经网络的神经元层数不能太多，一旦网络层数过多，就会使得网络无法训练，具体原因在后面的章节中会详细说明。Hopfield 神经网络，玻尔兹曼机以及受限玻尔兹曼机由于目前已经较少使用，所以本书后面章节不再详细介绍这三种网络。\\n\\n\\n\\n1.5.3 深度学习-2006 年至今', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1.5.3 深度学习-2006 年至今\\n\\n\\n\\n2006 年，多伦多大学的教授 Geoffrey Hinton 提出了深度学习。他在世界顶级学术期刊《Science》上发表了一篇论文《Reducing the dimensionality of data with neural networks》[7]，论文中提出了两个观点：①多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原始数据有更本质的代表性，这将大大便于分类和可视化问题；②对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。\\n\\n\\n\\nHinton 在论文中提出了一种新的网络结构深度置信网络（Deep Belief Net：DBN），这种网络使得训练深层的神经网络成为可能。深度置信网络由于目前已经较少使用，所以本书后面章节不再详细介绍这种网络。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='2012 年，Hinton 课题组为了证明深度学习的潜力，首次参加 ImageNet 图像识别比赛，通过 CNN 网络 AlexNet 一举夺得冠军。也正是由于该比赛，CNN 吸引了众多研究者的注意。\\n\\n\\n\\n2014 年，香港中文大学教授汤晓鸥领导的计算机视觉研究组开发了名为 DeepID 的深度学习模型， 在 LFW (Labeled Faces in the Wild，人脸识别使用非常广泛的测试基准)数据库上获得了 99.15%的识别率，人用肉眼在 LFW 上的识别率为 97.52%，深度学习在学术研究层面上已经超过了人用肉眼的识别。\\n\\n\\n\\n2016 年 3 月人工智能围棋比赛，由位于英国伦敦的谷歌（Google）旗下 DeepMind 公司的开发的 AlphaGo 战胜了世界围棋冠军、职业九段选手李世石，并以 4:1 的总比分获胜。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='2018 年 6 月，OpenAI 的研究人员开发了一种技术，可以在未标记的文本上训练 AI，可以大量减少人工标注的时间。几个月后谷歌推出了一个名为 BERT 的模型，该模型在学习了几百 万 个 句 子以后学会了 如 何 预测漏 掉 的 单 词 。 在 多 项 NLP (Natural Language Processing) 测试中，它的表现都接近人类。\\n\\n\\n\\n2020 年 6 月，OpenAI 发布了有史以来最大的 NLP 模型 GPT-3，GPT-3 模型参数达到了 1750 亿个参数，模型训练花费了上千万美元。GPT-3 训练方法很简单，但是却非常全能，可以完成填空，翻译，问答，阅读理解，数学计算，语法纠错等多项任务。随着 NLP 技术的发展，相信在将来 AI 可以逐渐理解我们的语言，跟我们进行顺畅的对话，甚至成为我们的保姆，老师或朋友。\\n\\n\\n\\n今天，人脸识别技术已经应用在了我们生活的方方面面，比如上下班打卡，飞机高铁出行，出门住酒店，刷脸支付等。我们已经离不开深度学习技术，而深度学习技术仍在快速发展中。\\n\\n\\n\\n1.6 深度学习领域重要人物', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='1.6 深度学习领域重要人物\\n\\n\\n\\n深度学习领域有很多做出过卓越贡献的大师，下面简单介绍几位。前面的 3 位大师Geoffrey Hinton、Yann LeCun、Yoshua Bengio 江湖人称“深度学习三巨头”，为了表彰 3 位大师对于神经网络深度学习领域的贡献，2018 年计算机领域最高奖项图灵奖颁给了他们。\\n\\n\\n\\n1.Geoffrey Hinton\\n\\n\\n\\n英国出生的计算机学家和心理学家，以其在神经网络方面的贡献闻名。Hinton 是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。目前担任多伦多大学计算机科学系教授。2013 年 3 月加入 Google，领导 Google Brain 项目。Hinton 被人们称为“深度学习教父”，可以说是目前对深度学习领域影响最大的人。而且如今在深度学习领域活跃的大师，有很多都是他的弟子，可以说是桃李满天下。\\n\\n\\n\\n2.Yann LeCun', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='2.Yann LeCun\\n\\n\\n\\n法国出生的计算机科学家，他最著名的工作是光学字符识别和计算机视觉上使用卷积神经网络（CNN），他也被称为卷积网络之父。曾在多伦多大学跟随 Geoffrey Hinton 做博士后。1988 年加入贝尔实验室，在贝尔实验\\n\\n室工作期间开发了一套能够识别手写数字的卷积神经网络系统，并把它命名为 LeNet。这个系统能自动识别银行支票。\\n\\n2003 年去了纽约大学担任教授，现在是纽约大学终身教授。\\n\\n2013 年 12 月加入了 Facebook，成为 Facebook 人工智能实验室的第一任主任。\\n\\n\\n\\n3.Yoshua Bengio\\n\\n\\n\\n毕业于麦吉尔大学，在 MIT 和贝尔实验室做过博士后研究员，自 1993 年之后就在蒙特利尔大学任教。在预训练问题，自动编码器降噪等领域做出重大贡献。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='这“三巨头”中的前两人早已投身工业界，而 Bengio 仍留在学术界教书，他曾说过：“我留在学术圈是为全人类作贡献，而不是为某一公司赚钱”。他说这句话一定是因为他很有钱，开个玩笑。每个领域的发展不仅需要做前沿的研究，还需要不断培养新的新鲜血液加入到这个行业中，所以如果大学教授都去工作的话，上课教书的人就少了。所以 Bengio 能留在学术圈，对行业的发展也是一件好事。\\n\\n2017 年初 Bengio 选择加入微软成为战略顾问。他表示不希望有一家或者两家公司（他指的显然是 Google 和 Facebook）成为人工智能变革中的唯一大玩家，这对研究社区没有好处，对人类也没有好处。\\n\\n\\n\\n4.Andrew Ng（吴恩达）\\n\\n\\n\\nAndrew Ng 是美籍华人，曾经是斯坦福大学计算机科学系和电气工程系的副教授，斯坦福人工智能实验室主任。他还与 Daphne Koller 一起创建了在线教育平台 Coursera。\\n\\n2011 年，Andrew Ng 在 Google 创建了 Google Brain 项目，通过分布式集群计算机开发超大规模的人工神经网络。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='2014 年 5 月，Andrew Ng 加入百度，负责百度大脑计划，并担任百度公司首席科学家。\\n\\n2017 年 3 月，Andrew Ng 从百度离职，目前自己创业。\\n\\nLeCun 是 Hinton 的 博士生，另一位人工智能大师 Jordan 曾经申请过 Hinton 的博士生，Bengio 是 Jordan 的博士后，Andrew Ng 是 Jordan 的博士生，LeCun 与 Bengio 曾经是同事。这个圈子很小，大家都认识，这几位大师互相之间有着很深的渊源。\\n\\n\\n\\n曾经神经网络的圈子很小，基本上入了这个圈以后就没什么前途了。正是由于这个圈子里的这些大师前辈们的不懈努力，把神经网络算法不断优化，才有了今天的深度学习和今天人工智能的新局面。\\n\\n\\n\\n1.7 新一轮人工智能爆发的三要素\\n\\n\\n\\n这一轮人工智能大爆发的主要原因有 3 个，深度学习算法，大数据，以及高性能计算。深度学习算法 —— 之前人工智能领域的实际应用主要是使用传统的机器学习算法，虽然这些传统的机器学习算法在很多领域都取得了不错的效果，不过仍然有非常大的提升空间。', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='深度学习出现后，计算机视觉，自然语言处理，语音识别等领域都取得了非常大的进步。大数据 —— 如果把人工智能比喻成一个火箭，那么这个火箭需要发射升空，它的燃料就是大数据。以前在实验室环境下很难收集到足够多的样本，现在的数据相对以前在数量、覆盖性和全面性方面都获得了大幅提升。一般来说深度学习模型想要获得好的效果，就需要把大量的数据放到模型中进行训练。\\n\\n\\n\\n高性能计算 —— 以前高性能计算大家用的是 CPU 集群，现在做深度学习都是用GPU(Graphics Processing Unit)或 TPU(Tensor Processing Unit)。想要使用大量的数据来训练复杂的深度学习模型那就必须要具备高性能计算能力。GPU 就是我们日常所说的显卡，平时主要用于打游戏。但是 GPU 不仅可以用于打游戏，还可以用来训练模型，性价比很高，买显卡的理由又多了一个。如果只是使用几个 CPU 来训练一个复杂模型可能会需要花费几周', metadata={'source': 'test.docx'}),\n",
       " Document(page_content='甚至几个月的时间。把数百块 GPU 连接起来做成集群，用这些集群来训练模型，原来一个月才能训练出来的网络，可以加速到几个小时甚至几分钟就能训练完，可以大大减少模型训练时间。TPU 是谷歌专门为机器学习量身定做的处理器，执行每个操作所需的晶体管数量更少，效率更高。\\n\\n\\n\\n工欲善其事，必先利其器。下一章节我们将介绍如何搭建 python 开发环境，为我们后续的学习做准。', metadata={'source': 'test.docx'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs3 = text_splitter.split_documents(documents3)\n",
    "split_docs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9927486d-96ba-44cf-b533-b9aef1894a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:36:47.663141Z",
     "iopub.status.busy": "2023-12-01T02:36:47.663141Z",
     "iopub.status.idle": "2023-12-01T02:36:47.672002Z",
     "shell.execute_reply": "2023-12-01T02:36:47.671477Z",
     "shell.execute_reply.started": "2023-12-01T02:36:47.663141Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_docs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6117a5a-c01f-4160-9178-902d6413fb2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:54:36.513270Z",
     "iopub.status.busy": "2023-12-01T02:54:36.513270Z",
     "iopub.status.idle": "2023-12-01T02:54:36.518905Z",
     "shell.execute_reply": "2023-12-01T02:54:36.518763Z",
     "shell.execute_reply.started": "2023-12-01T02:54:36.513270Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs3[0].metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "908cd6f0-d27f-49c1-8ebf-898caafd15ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:56:00.171957Z",
     "iopub.status.busy": "2023-12-01T02:56:00.171957Z",
     "iopub.status.idle": "2023-12-01T02:56:00.193954Z",
     "shell.execute_reply": "2023-12-01T02:56:00.193954Z",
     "shell.execute_reply.started": "2023-12-01T02:56:00.171957Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Docx2txtLoader.__init__() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Docx2txtLoader\n\u001b[1;32m----> 3\u001b[0m loader4 \u001b[38;5;241m=\u001b[39m \u001b[43mDocx2txtLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.docx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melements\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m documents4 \u001b[38;5;241m=\u001b[39m loader4\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      5\u001b[0m documents4\n",
      "\u001b[1;31mTypeError\u001b[0m: Docx2txtLoader.__init__() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader4 = Docx2txtLoader(\"test.docx\", mode=\"elements\")\n",
    "documents4 = loader4.load()\n",
    "documents4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc5d03-8d95-45ef-8a50-d205a80ca7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-env",
   "language": "python",
   "name": "chat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
